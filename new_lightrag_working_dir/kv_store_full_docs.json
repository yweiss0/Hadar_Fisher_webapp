{
  "doc-e5aad077e5e231333785f503196a76e5": {
    "content": "1Using Natural Language Processing to Track Negative Emotions in the Daily Lives of \nAdolescents   \n 2Abstract \nTracking emotion fluctuations in adolescents’ daily lives is essential for understanding mood dynamics \nand identifying early markers of affective disorders. To uncover the utility of text-based approaches for \nemotion prediction, this study compares nomothetic and idiographic modeling approaches for \npredicting adolescents’ daily negative affect (NA) using text features. It also evaluates different \nNatural Language Processing (NLP) techniques in capturing within-person emotion fluctuations. We \nanalyzed ecological momentary assessment (EMA) text responses from 98 adolescents (ages 14-18, \n77.3 % female, 22.7% male, N EMA=7,680). Text features were extracted using dictionary-based \napproach, topic modeling, and GPT-derived emotion ratings. Random Forest and Elastic Net \nRegression models predicted NA from these text features, comparing nomothetic (group-level) and \nidiographic (individualized) approaches. All key findings, interactive visualizations, and model \ncomparisons are available via the website: https://emotracknlp.streamlit.app/ . Idiographic models \ncombining text features from different NLP approaches showed the best performance: they performed \ncomparably to nomothetic models in R² but yielded lower prediction error (Root Mean Squared Error), \nimproving within-person precision. Importantly, there was substantial between-person differences in \nmodel performance and predictive linguistic features. When selecting the best-performing model for \neach participant, significant correlations between predicted and observed emotion scores were found \nfor 90.7–94.8% of participants. Our findings suggest that while nomothetic models offer initial \nscalability, idiographic models may provide greater predictive precision with sufficient within-person \ndata. A flexible, personalized approach that selects the optimal model for each individual may enhance \nemotion monitoring, while leveraging text data to provide contextual insights. \n \nEmotions play a fundamental role in human life, serving as essential cues that influence our \nactions and interactions with the environment 1. Emotional states act as immediate alerts to potential \nbenefits or dangers, driving us towards actions that align with our personal goals and away from Commented [HF1]: The first part (excluding the method) \nup to 5000 words. No title to the introduction.  \n 3potential threats 2. Research shows that not only the intensity of emotions but also their temporal \ndynamics can contribute to the development of mood disorders 3–7. This is especially important for \nadolescents, a developmental period marked by increases in the frequency of negative emotional states, \nwhich may contribute to their elevated risk for depression 8–10. Developing effective methods to \nmeasure daily emotional changes that are specifically suitable for adolescents is crucial. These \nmeasurements could identify both acute emotional distress and longer-term maladaptive patterns in \nemotions, serving two key purposes: enabling timely interventions during periods of high negative \naffect and facilitating targeted intervention to address maladaptive emotional patterns. Such methods \ncould help predict increased risk of affective disorder onset (such as depression), which could inform \nthe timely and targeted delivery of preventative interventions, ultimately contributing to improved \nmental health outcomes.  \nLanguage offers a promising avenue for measuring emotion, as people convey their emotional \nstates both explicitly and implicitly through their choice of words and their manner of speaking or \nwriting 11,12. Language plays a dual role in the realm of emotions, serving not only as a primary tool for \nexpressing and communicating emotional states but also as an actively shaping how emotions are \nexperienced 13,14. Therefore, the current study aims to explore whether text can be used to track within-\nperson fluctuation in emotional states. \nWith the rise of digital communication, people, especially adolescents, now express themselves \nextensively through text-based interactions, including social media posts, messages, and online \ndiscussions. This shift has generated an unprecedented volume of language data, offering a unique \nopportunity to study adolescent emotions on a larger scale 15,16. Advances in natural language \nprocessing (NLP), a multidisciplinary field combining computer science, artificial intelligence, and \nlinguistics, have enabled researchers to analyze this data efficiently and meaningfully 17,18. While NLP \nhas been used in behavioral science for decades, its accessibility and affordability have significantly \n 4improved, making it an increasingly popular tool 19. One key advantage of NLP over traditional, \nmanual text analysis is its ability to handle large datasets—such as thousands of social media posts or \ndigitized texts—quickly and efficiently. By leveraging these advancements, NLP methods can identify \npatterns in language use (e.g., sentiment, tone, and self-referential quality), offering real-time insights \ninto emotional states without requiring participants to actively report their feelings 20–22. Importantly, \ntext-based communication provides insights not only into the intensity of emotions but also into the \ncontext in which those emotions arise. This contextual information can reveal situational factors \ncontributing to emotional experiences, offering a more nuanced understanding of emotional processes.  \nIn recent years, various text analysis tools have emerged 23–29, ranging from basic approaches \nlike counting word frequencies to more advanced methods, such as leveraging large language models \n(LLMs), each with its own set of advantages and limitations. For example, closed-vocabulary \nprograms (i.e., dictionary-based approach)1 such as the Linguistic Inquiry and Word Count (LIWC; \nPennebaker, 2001), use predefined dictionaries to categorize words. While highly interpretable, \ntransparent, and efficient at summarizing concepts, these methods often neglect context, leading to \npotential misinterpretations 25. LIWC may classify the word mad as an indicator of anger, yet in \nphrases like \"She’s mad talented\" , it conveys emphasis rather than negativity, illustrating how context-\ndependent meanings can lead to inaccurate classifications. In contrast, open-vocabulary approaches, \nsuch as Latent Dirichlet Allocation (LDA), and word embedding methods, leverage data-driven \ntechniques to examine a broader spectrum of words and topics 31–33. These methods are better at \ncapturing nuances, addressing ambiguous word meanings, and are less susceptible to \nmisinterpretations. Limitations of open-vocabulary approaches include the need for more technical \n \n1 We use the terms “closed-vocabulary” and “dictionary-based” interchangeably to refer to approaches that rely on \npredefined word categories (e.g., LIWC). These methods contrast with open-vocabulary approaches, which derive features \ndirectly from the text data without relying on preset dictionaries, often using machine learning or statistical models to \nidentify relevant patterns. \n 5expertise, larger datasets, and careful consideration of parameter choices, as well as challenges in \ninterpretability 25,29.  \nMore recently, LLMs like GPT have shown great promise in accurately identifying various \npsychological constructs in text, overcoming many of the constraints of older methods (Rathje et al., \n2024). These models can interpret the context of words and have achieved effective results across \nmultiple languages with simple prompts. The main limitations of this approach include a lack of \ntransparency in how inferences were generated and difficulties in reproducing results due to their \nprobabilistic nature 34. For example, although LLM-based approaches could predict whether a person \nexperiences negative emotions based on social media posts, LLMs could not be used to identify which \nlinguistic cues (such as the use of pronouns) predict this experience 19,27. In the latter case, dictionary-\nbased methods are preferrable. Ultimately, closed- and open-vocabulary approaches, along with \nadvancements in LLMs, provide complementary strengths that significantly enhance our ability to \nunderstand psychological states through language. By combining these methods, researchers may be \nable to leverage their unique advantages while mitigating their limitations, offering a robust alternative \nto traditional and relatively cumbersome self-report measures assessing emotions. \nDespite the increasing sophistication of these methods, significant gaps remain in the current \nliterature. Most importantly, existing studies on text-based emotion prediction have focused on \nexploring between-person differences, primarily identifying which text features are associated with \nindividuals experiencing high levels of negative emotions (e.g., Tackman et al., 2019). These \napproaches aggregate data across participants and aim to pinpoint common linguistic markers, such as \nthe use of first-person pronouns or negative sentiment, that are indicative of heightened emotional \ndistress and depression 36,37. However, this method overlooks both between-person differences in how \nlinguistic markers relate to emotions and within-person fluctuations in emotional states. Even when \nusing multilevel models to predict within-person fluctuations in emotions, models assumes that the \nsame linguistic features signal high levels of negative emotions across different individuals 37.  \n 6Such assumptions fail to account for the fact that language is inherently idiosyncratic, with \nindividuals expressing emotions in unique and context-dependent ways. Beck & Jackson, (2022) \ndemonstrated the importance of idiographic approaches by showing that the psychological and \nsituational antecedents that predicted future loneliness varied substantially across participants, with no \ntwo individuals showing the same pattern of predictive features. This highlights the need for \npersonalized models that adapt to the distinctive ways people express emotions through text, enabling \nmore accurate predictions tailored to each individual. To address this limitation, this study integrates \ndiverse language-based tools and leverages machine learning to develop personalized models capable \nof monitoring within-person fluctuations in emotional states.  \nThe study's primary goal is to examine whether within-person fluctuations in negative affect \n(NA) in adolescents' daily lives can be accurately tracked through text analysis. Since individuals \nexpress emotions in a variety of linguistic ways, this research aims to develop more personalized \napproaches to emotion tracking. \nSpecifically, we address two key questions: \n1) Do idiographic (individual-level) models outperform nomothetic (group-level) models in \npredicting emotion fluctuations? This question is motivated by the fact that individuals may \nexpress emotions through language in unique ways. \n2) Is combining various NLP approaches improves emotion prediction? \nWe compare three types of Natural Language Processing (NLP) approaches, which, as noted \nabove, have complementary strengths and limitations: \na) Closed vocabulary (dictionary-based approach: LIWC and VADER) \nb) Open vocabulary (LDA) \nc) GPT-derived ratings \nBy addressing these questions, the study aims to enhance the accuracy of emotion prediction \nmodels, potentially enabling closer monitoring of emotional fluctuations in daily life. The ultimate \n 7goal is to use these improved emotion predictions to inform the delivery of scalable, real-time, and \npersonalized interventions for alleviating high NA states and enhancing emotion regulation abilities in \nyouth. \nResults \nThe full results can be explored in the web app ( https://emotracknlp.streamlit.app/ ), which \nalso includes an interactive chatbot explaining the research, the figures and results presented in this \npaper, as well as additional findings. We encourage readers to explore the web app to engage more \ndeeply with the data and insights. \nDemographic and clinical characteristics \nTable 2 presents the demographic and clinical characteristics of the participants. For the \nparticipants who completed EMA in four consecutive weeks, the median number of EMA surveys \ncompleted was 77 (M = 75, SD = 24), and mean EMA compliance was 67% (SD = 21%).  For the \nparticipants who completed EMA in biweekly 5-day blocks, the median number of EMA surveys \ncompleted was 66 (M = 68, SD = 24), and mean EMA compliance was 51% (SD = 17%).  \nModel performance: Nomothetic Approach \nTable 3 presents model performance metrics for predicting negative affect using combined text \nfeatures from three NLP techniques. Three modeling approaches are compared: group-level \nnomothetic (one model combining all observations), per-participant nomothetic (performance \ncalculated separately), and idiographic (separate models for each individual). Further details and \nresults, including separate models for each NLP technique, are available in the Web app (see \"Model \nperformance across sample” and “Model performance per participant”). \nThe group-level nomothetic model performed well, with R² values ranging from 0.11 to 0.38. \nHowever, when evaluated separately for each participant, which is critical given the ultimate goal of \ntracking fluctuations in emotions within individuals, performance decreased substantially. Mean R² \nvalues fell between 0.06 and 0.11, with considerable variability in predictive accuracy across \n 8participants. For example, when predicting mean negative affect, individual R² values ranged from \n0.00 to 0.41, demonstrating that while text features explained little to no variability in negative affect \nfor some, they accounted for a significant proportion of variability in others. See Table 3 for results for \nthe specific emotions of sadness, anger, and nervousness. \nModel performance: Idiographic Approach \nThe idiographic approach (separate models for each individual) showed similar average \nperformance to the nomothetic approach, with mean R² values ranging from 0.06 to 0.10. However, the \nidiographic approach demonstrated better performance in terms of RMSE (0.47 to 0.76) compared to \nthe nomothetic model (0.52 to 0.88), suggesting lower prediction error. In addition, for negative affect, \nthe idiographic approach showed greater variability in R² across participants, achieving higher \naccuracy for some individuals. However, the nomothetic approach identified significant associations in \na larger proportion of participants (55–58% vs. 43–48%). A similar pattern emerged for sadness, \nsuggesting that while person-specific models can improve accuracy for certain individuals, the \nnomothetic approach is more robust for detecting significant associations across a broader sample. Fig. \n2 displays the relationship between predicted and observed (actual) negative affect ratings using \nperson-specific (idiographic) models. Each colored line represents an individual participant’s predicted \nestimates (y axis) across different levels of actual negative affect (x axis). While the overall trend, \nrepresented by the dashed black line, suggests a generally positive relationship between predicted and \nactual ratings, indicating that the models capture meaningful within-person fluctuations in negative \naffect, this should not be taken to imply consistent accuracy at the individual level. The notable \nbetween-person differences in model fit underscore that good overall performance does not guarantee \nreliable prediction for every participant. \nFig. 3 displays the substantial differences in model accuracy across individuals by showcasing \nexamples of four high- vs low-accuracy person-specific models (figures for all other outcomes and for \n 9all participants for both the nomothetic and idiographic models are available on the Web app “True vs. \nPredicted”). \nExamining Between-Person Differences in Which Text Features Predict Negative Affect \nFig. 4 illustrates the variability in feature importance by showing the top 10 text features from \nthe four best-performing subject-specific random forest models predicting negative affect. These \nmodels are the same four high-accuracy models displayed in Fig. 3. The results show two types of \nvariability between individuals: differences in model performance and, even among highly accurate \nmodels, differences in the text features that contribute to predicting negative affect. Critically, this \nvisualization demonstrates how text features provide valuable contextual information. For example, \none participant (K23541) exhibits more negative emotions when using words related to their family \n(\"Family\"), yet another (KTGF533) shows greater negative emotions when focusing on the past \n(\"FocusPast\"). Similarly, a different participant (KTGF528) expresses more negative emotions when \nwriting about work-related topics (\"Work\") but demonstrates less negativity when using words \nassociated with acquiring objects, states, or goals that fulfill personal needs (\"Acquire\"). This rich \ninformation can be used to create person-specific profiles that enable us to identify the specific \ncontexts in which negative emotions arise, providing insights that can be used to personalize \ninterventions. A heatmap of the feature importance across participants and feature importance figures \nfor each participant, outcome, and ML model are available on the Web app (\"Feature importance \nheatmap\" and \"Feature importance per participant\"). Details on the differences in predictive text \nfeatures between high- and low-performing models can be found in the Supplement ( Section S4  and \nFig. S3). \nDo Closed Vocabulary, Open Vocabulary, LLM, or Combined Approaches Perform Best in \nPredicting Negative Emotions? \nFig. 6 displays the performance metrics for each NLP approach used in the idiographic models. \nWhen examining the idiographic predictive performance of each NLP approach separately, GPT \n 10generally showed better performance for negative affect, sadness, anger, and nervousness, with R² \nvalues around 0.10 for negative affect and sadness, and slightly lower for anger and nervousness. \nThese results were comparable to those achieved by the idiographic models that combined all NLP \napproaches (see Table 3 for the combined model results), except in the prediction of nervousness \nwhere GPT slightly outperformed the combined idiographic model. However, it is important to note \nthat while GPT had relatively strong R² values, it also had a higher RMSE compared to the \ncombined models, suggesting a higher average prediction error despite accounting for more \nvariance in the outcomes. LIWC+VADER and LDA each demonstrated poorer predictive \nperformance, with the lowest average R² values and fewer significant associations across all \nemotions, especially for anger and nervousness. This highlights that the contextual understanding \nprovided by GPT was more effective in capturing within-person emotional fluctuations compared to \nthe other methods when NLP approaches were tested individually. Overall, while GPT captures \nvariability in negative affect (as reflected by R²), its predictions are less precise in terms of exact \nvalues (as indicated by RMSE). Combined models, which balance capturing variability and \nminimizing prediction errors, offer a more robust and reliable approach to emotion prediction. \nEvaluating Best Performing Models for Each Individual \n          To investigate the effectiveness of personalized models, we identified the best-performing \npredictive model for each of the 97 participants. Using this flexible approach, the vast majority (90.7-\n94.8%) of the participants showed a significant correlation between predicted and observed (actual) \nemotion scores. The table detailing the best model for each participant and each emotion outcome can \nbe found in the Web app under \"Best model performance\". Fig. S4 shows the distribution of best-\nperforming models across participants. Idiographic models were most frequently selected for all \noutcomes except for Anger, where nomothetic models performed best for 44.3% of the participants. \nElastic Net showed the best performance for most participants across outcomes, followed by Random \nForest. For NLP approaches, the model combining all text features outperformed for most of the \n 11participants (52.6-59.8%). Interestingly, though GPT showed the highest mean R² scores, it was \nchosen as the best model for only a relatively small portion of the participants (11.3-19.6%). \nAre Models Emotion-Specific? \nTo examine whether our models captured emotion-specific signals rather than general negative \naffect, we compared the extent to which each emotion-specific model (both Random Forest and Elastic \nNet) predicted each observed emotion. Specifically, for each observed emotion outcome (NA, Sad, \nAngry, and Nervous), we calculated R² and RMSE between the observed and the predicted emotion, \neither matching (e.g., observed and predicted sadness) or mismatching (e.g., observed sadness and \npredicted anger). As shown in Fig. S4-S7 (see also in the web app under “Best Model Performance”), \nwhich display box plots of the distribution of R² and RMSE for each emotion, overall, and consistent \nwith the interpretation that the models have emotion-specific signals, the matched predictions showed \nhigher R² and lower RMSE than mismatched predictions, indicating that the models capture unique, \nemotion-specific variance. However, for RMSE, we sometimes observed better performance for NA \nrelative to the specific emotions, suggesting that the signals underlying broad negative affect might be \nmore robust or reliable relative to single emotion ratings. \nDiscussion  \nIn this study, we combined multiple Natural Language Processing (NLP) approaches to examine \nwhether within-person fluctuations in emotion can be accurately tracked through text analysis. \nRecognizing the idiosyncratic nature of emotional communication, we compared idiographic models, \ntailored to individual patterns, with nomothetic models that capture common trends across groups.  The \nleveraging of advanced NLP and machine learning techniques to track moment-to-moment emotional \nchanges through text analysis has the potential to enhance mental health monitoring, support clinical \ndecision-making, and enable early detection of distress for timely, personalized interventions. \nThe results showed that, overall, group-level nomothetic models showed high performance in \ncontinuously predicting negative emotions (R2 range: 0.11-0.38). These findings align with previous \n 12studies demonstrating the utility of NLP approaches in detecting emotional states across participants \n39,40. However, since these studies primarily focused on identifying emotional states at the between-\nperson level, rather than capturing within-person  fluctuations, their high performance does not \nnecessarily indicate an ability to accurately track moment-to-moment emotion fluctuation within \nindividuals. When we calculated performance metrics for each participant individually, the mean \nmodels' performance declined significantly and showed high between-person variability, indicating \nthat the nomothetic models' ability to track within-person changes varies considerably from one \nindividual to another. \nWhen we built separate models for each participant (idiographic models) and compared their \nperformance to that of nomothetic we found that the mean R2 was comparable between the two \napproaches, however, RMSE was lower for the idiographic models. In addition, idiographic models \nrevealed significant between-person variability in the text features predicting emotional states, \nindicating that individuals express negative emotions in distinct linguistic ways. This raises an \nintriguing question: if individuals differ so markedly in their predictive features, how can a nomothetic \nmodel that captures only general trends perform just as well? One plausible explanation is a trade-off \nbetween statistical power and individual variation. While idiographic models capture unique, person-\nspecific patterns, nomothetic models benefit from larger datasets that enable the estimation of multiple, \nweaker yet common feature-emotion relationships. In other words, although individual differences \nexist, the robust common patterns identified by the nomothetic approach appear sufficient to achieve \nsimilar predictive accuracy. Supporting this interpretation, idiographic models showed lower RMSE \nand higher accuracy for more participants compared to the nomothetic models. However, nomothetic \nmodels demonstrated greater consistency across participants, with more participants presenting \nsignificant associations between predicted and actual negative emotions compared to idiographic \nmodels.  \n 13Rather than directly comparing nomothetic and idiographic models, an alternative approach is to \nselect the best-performing model for each participant. Using this flexible approach, the vast majority \n(90.7–94.8%) of participants showed a significant correlation between predicted and observed emotion \nscores, highlighting the potential of personalized modeling strategies to improve prediction accuracy. \nWhile we are not aware of studies directly comparing nomothetic and idiographic approaches in \ntext analysis, previous research comparing their effectiveness in tracking mental states using passive \nsensor data from smartphones and actigraphy, as well as self-reported ecological momentary \nassessment (EMA), has yielded similar results (e.g., Aalbers et al., 2023; Cheung et al., 2017; Rozet et \nal., 2019; for exception see Soyster et al., 2022 who found that nomothetic models outperform \nidiographic models). For example, Aalbers et al., 2023 used smartphone passive sensor data to predict \nstress levels. Their findings revealed that idiographic models demonstrated higher accuracy in tracking \nstress levels for some participants (Spearman’s ρ rank-order correlation up to 1 in idiographic models \nvs. up to .65 in nomothetic models). However, nomothetic models significantly predicted stress for a \nlarger proportion of participants (up to 23.2% for idiographic models vs. up to 55% for nomothetic \nmodels). Rozet et al. (2019) used a comparable method and initially found that nomothetic models \nperformed better (i.e., were more accurate) than idiographic models. However, as more data \naccumulated, the performance of the idiographic model eventually equaled and then surpassed that of \nthe nomothetic model, suggesting that idiographic models may be a better option when sufficient data \nis available at the single subject level.   \nWhen comparing specific NLP approaches, GPT outperformed other models in its ability to track \nfluctuations in emotional states. It achieved R² values comparable to the model combining all three \nNLP approaches. Despite the relatively high R² values, GPT predictions also showed higher RMSE \nscores compared to the combined models, suggesting that while GPT may account for meaningful  \nvariance in emotional states, it may be less precise in predicting the exact numerical values of these \nstates. Furthermore, when selecting the best-performing model for each participant, GPT was chosen \n 14for only a relatively small portion of individuals, indicating that its predictions are  relatively stable \nacross individuals but not necessarily the most accurate for any single participant. This pattern aligns \nwith the nomothetic-idiographic trade-off discussed earlier, as GPT operates as a nomothetic model \nthat generalizes across individuals rather than incorporating person-specific information. Future \nresearch should aim to optimize this balance by leveraging fine-tuning techniques, such as participant-\nspecific calibration or supervised fine-tuning on emotion-labeled data,  to improve GPT model \naccuracy.  \nFrom an applied perspective, GPT is already trained on a vast corpus of data (approximately 45 \nterabytes of text data from various sources 45) which makes it particularly attractive for studies with \nlimited datasets, as it does not necessarily require retraining on task-specific data to yield meaningful \npredictions. However, the model's lack of transparency and inability to explicitly articulate the \nrationale behind its predictions remain critical limitations that researchers must carefully weigh 19. \nGiven these considerations, we recommend that researchers thoughtfully evaluate GPT's role in their \nanalytical pipeline, considering whether to employ it as a standalone tool or integrate it with \ncomplementary approaches. Future work should focus on developing methods that combine GPT's \npowerful inference capabilities with more transparent analytical approaches, potentially offering a \nmore robust framework for emotion analysis in psychological research. \nLimitation and future directions \nThis study is the first to use different NLP approaches to directly compare nomothetic and \nidiographic models for tracking emotional fluctuations. Yet, findings of this study should be \ninterpreted in light of several limitations. First, capturing the nuanced dynamics of emotional \nexpression typically requires large amounts of data. In everyday interactions, subtle emotional changes \nmay only become apparent with extensive exposure to an individual’s language use, much like \nknowing someone well enables you to discern small shifts in their mood. Therefore, future studies \n 15should strive to collect larger within-person datasets, potentially sourced from daily-life \ncommunications, to enhance the ability to track and model these nuances. \nSecond, the overall predictive accuracy was modest, leaving significant room for improvement. \nIn addition to increasing the quantity of data, future research could benefit from incorporating \nadditional modalities (e.g., vocal features, facial expression, passive sensors from smartphones or \nwearables such as smartwatches or smart ring, which are becoming increasingly popular) alongside \ntext. Prior studies have shown that combining various modalities can enhance the performance of \nnomothetic models of emotion (see Gandhi et al., 2023 for a review), this multimodal approach may \nalso improve idiographic models. Moreover, future work should explore whether individuals not only \ndiffer in the text features that predict their emotions but also in the types of modalities that best capture \ntheir emotional states.  For example, individual differences, such as tendency to use affective \nsuppression, may influence how emotions are conveyed in text, with greater suppression potentially \nleading to lower model performance. Future studies should examine whether these participants require \nlarger datasets to capture subtle nuances in their text, or if alternative modalities (e.g., passive sensor) \nbetter detect changes in their emotional states. \nThird, the current study compared idiographic and nomothetic approaches, demonstrating that \neach has its merits. Future studies could explore hybrid approaches that integrate both individual-\nspecific and group-level information, balancing personalization with statistical power to enhance \npredictive accuracy. Finally, the text data in our study consisted of responses to specific questions, \nwhich may limit the generalizability of our findings to other types of text (e.g., text messages and \nsocial media posts). Future research should incorporate text from diverse sources such as social media, \nconversational exchanges, and free-form writing, to determine whether these results extend to broader \ncontexts.  \n 16Conclusion \nIn conclusion, our findings highlight the potential of combining NLP approaches to track within-\nperson emotional fluctuations, demonstrating both the strengths and limitations of idiographic and \nnomothetic models. Nomothetic models effectively capture general trends, making them useful for \nbroad applications. In contrast, idiographic models provide a more nuanced understanding by \nidentifying person-specific features that reflect an individual's unique emotional expression, though \nthey require large within-person datasets to model these patterns reliably. Rather than a one-size-fits-\nall approach, our results suggest that selecting the best approach for each participant can enhance \npredictive accuracy. Expanding this work to incorporate diverse text sources and multimodal data \nstreams (e.g., integrating social media posts, speech transcripts, facial expressions, physiological \nsignals, or smartphone sensor data) may further advance the field. \nUltimately, improving the ability to monitor emotions in real-time not only enhances our ability \nto study emotional phenomena as they unfold in daily life but also can help inform the development \nand deployment of just-in-time (JIT) interventions that go beyond identifying moments of distress to \nalso consider the specific emotions and contextual factors in which they arise, enabling more \npersonalized and effective interventions for youth. \n 17Method \nParticipants \nParticipants were derived from two larger studies that recruited adolescents with elevated levels \nof anhedonia, as well as typically developing (non-anhedonic) adolescents. They included 97 English-\nspeaking adolescents aged 12-18 (75 female, 22 male; M age = 16.2, SD = 1.9) recruited from the \ngreater Boston area. Participants were excluded based on a history or current diagnosis of any of the \nfollowing DSM-5 psychiatric illnesses: schizophrenia spectrum or other psychotic disorder, bipolar \ndisorder, substance or alcohol use disorder within the past 12 months or lifetime severe substance or \nalcohol use disorder. Participants were also excluded based on current diagnosis of anorexia nervosa or \nbulimia nervosa or if they had a neurodevelopmental disorder that would interfere with study tasks. \nDue to the neuroimaging component of this study, additional exclusion criteria included fMRI \ncontraindications. For additional information about sample inclusion and exclusion criteria see Murray \net al., (2023).                                                                                                             \nProcedure   \nAll procedures were approved by the Mass General Brigham IRB. Participants who were 18 \nyears of age provided written informed consent; participants who were under 18 provided written \nassent, with their parents providing written consent. At the baseline session, either in-person or over \nZoom, participants were administered a semi-structured clinical interview, the Kiddie Schedule for \nAffective Disorders and Schizophrenia (K-SADS; Kaufman et al., 1997), and completed self-report \nmeasures. Following the baseline session, participants installed the MetricWire app on their \nsmartphones to complete ecological momentary assessments (EMAs). Of the participants, 39 \ncompleted the EMA for four consecutive weeks; for the first 5 days, surveys were delivered 2-3 times \nper day in the afternoon and evening, and for the remaining 25 days, surveys were delivered 4 times \nper day. The other 58 participants completed the EMA in 5-day (Thursday through Monday) blocks \nevery other week, with 2-3 survey prompts per day, for a median of 17 weeks.  \n 18Measures \nEcological Momentary Assessment (EMA)  \nParticipants were asked to rate on a 5-point Likert scale, ranging from 0, “Very slightly or not at \nall,” to 4, “Extremely” the extent to which they were feeling several emotions immediately before they \nstarted the assessment. Participants’ negative affect (NA) included responses for: “sad,” “nervous,” \nand “angry.” Mean NA was measured by averaging the three NA variables. In addition, participants \nresponded to the following open-ended questions: 1) What were you thinking about right before you \nstarted this survey? 2) Think about the most enjoyable or happy time since you completed the last \nsurvey (or if this is your first survey, then the last 24 hours). Very briefly, what happened (1-2 \nsentences is fine)? 3) Think about the most stressful or negative time since you completed the last \nsurvey (or if this is your first survey, then the last 24 hours). Very briefly, what happened (1-2 \nsentences is fine)? Participants with fewer than 30 observations were excluded from the analysis.  \nLanguage Measures  \nWe used the following strategies to extract text features and generate quantitative summaries of \nthe language data. \nClose vocabulary.  For close vocabulary (dictionary-based approach), we extracted features \nusing the Linguistic Inquiry and Word Count (LIWC; Tausczik & Pennebaker, 2010) and the Valence \nAware Dictionary and sEntiment Reasoner (VADER; Hutto & Gilbert, 2014). LIWC is a computerized \ntext analysis tool that categorizes words into over 90 linguistic and psychological dimensions based on \nan internal dictionary of approximately 6,400 words 48. It calculates the percentage of words that match \neach predefined category, offering insights into linguistic structures (e.g., pronouns), psychological \nconstructs (e.g., affect), and broader language patterns (e.g., analytical thinking). The LIWC has been \nextensively validated across numerous studies and is widely used in psychological research to quantify \nlanguage use patterns associated with various psychological states and traits. For this study, we used \n 19LIWC-22 23, the latest version of the software, the analyze participants' responses to the EMA open \nquestions. \nVADER is a simple rule-based model for general sentiment analysis optimized for social media. \nIt provides four sentiment scores: negative, positive, neutral, and compound (an overall sentiment \nscore from -1 to +1). VADER is particularly effective at handling sentiments expressed in short, \ninformal text and accounts for factors like punctuation, capitalization, and modifiers (e.g., intensifiers \nlike \"very\") that influence the intensity of the sentiment. Whereas LIWC excels in offering detailed \npsychological and linguistic insights across a wide range of texts, VADER is more attuned to detecting \nsentiment polarity and intensity in social media. Finally, text lengths were extracted as features for \neach question and included in the models. \nOpen vocabulary.  Latent Dirichlet Allocation (LDA) is a probabilistic clustering method that \nidentifies topics based on word co-occurrence rather than relying on predefined dictionaries 31,32. This \napproach allows LDA to group semantically related words while considering context, reducing word \nsense ambiguity. The latent topics were extracted from the preprocessed text corpus using probabilistic \nmodeling with the R package topicmodels  49. In our study, we implemented LDA using two distinct \napproaches for nomothetic and idiographic analyses. For the nomothetic model, we extracted a \nuniform set of topics across the entire sample. For the idiographic model, we applied LDA separately \nfor each individual, identifying topics based on their unique word usage patterns. To interpret the \nextracted nomothetic  topics, Table 1 presents the 15 most frequent words per topic, offering a \nsemantic representation of the clustering. Furthermore, Fig. S2 depicts the distribution of LDA-derived \ntopics across participants, highlighting variations in topic prevalence within the dataset. Details on \ntopic selection, model tuning, and evaluation metrics are provided in the Supplement.  \nGenerative Pre-trained Transformer (GPT).  To generate emotion ratings using large \nlanguage models (LLMs), we used GPT, a deep learning-based AI language model developed by \nOpenAI. GPT is built on a transformer-based architecture, a neural network design that excels at \n 20processing sequential data by using self-attention mechanisms to capture contextual relationships \nacross words. This allows GPT to generate human-like text by predicting the next word in a sequence \nbased on the provided context. Specifically, we used GPT-4, an advanced version pre-trained on a vast \ndataset (45TB), enabling it to generate coherent sentences and perform various tasks such as writing, \nanswering questions, and engaging in conversations. In this study, we prompted GPT-4 through the \napplication programming interface (API) using Python code to rate the extent to which a participant \nexperienced one of the following emotions: Sadness, Anger, and Nervousness, on a scale of 1 to 5 \n(similar to the scale used in EMA), based on responses to three open-text questions. See the \nsupplement for the full prompt used to generate GPT responses. \nData Analysis \nModel Specification   \nWe began our analysis by extracting text data from EMA responses and conducting both lexicon-\nbased and transformer-based analyses. Next, we applied preprocessing steps (e.g., text normalization, \nstopword removal, stemming) to refine the text before performing topic modeling. Finally, we trained \nmachine learning models to predict negative affect based on the derived text features, evaluated model \nperformance, and determined feature importance. Fig. 1 illustrates this analysis pipeline in detail. \nWe compared both nomothetic (group-level) and idiographic (individual-level) models in their \nability to predict variability in negative emotional states within individuals over time. We employed \ntwo machine learning approaches to predict negative affect: elastic net regression and random forest \nmodels. Elastic net regularization (ENR) is a popular variant of conventional regression that combines \ntwo types of penalties: ridge and lasso. This combination helps address issues related to \nmulticollinearity by constraining the coefficients of correlated variables while also minimizing model \noverfitting. On the other hand, random forest (RF) is an ensemble learning method based on decision \ntrees. Unlike ENR, random forest can capture complex nonlinear relationships and interactions \nbetween variables without having to specify them in advance. Given that these two methods rely on \n 21different algorithms (i.e., penalized regression- vs decision tree-based approaches) for selecting \nvariables, assessing their importance, and generating predictions, comparing them can help to \ndetermine which one provides the most accurate predictions in a given context. \nTo assess model performance and generalizability while minimizing overfitting, we implemented \na nested cross-validation (CV) procedure using the nestedcv package in R (Lewis et al., 2023). A full \ndescription of the nested CV procedure is provided in section S3 in the Supplement. We evaluated \nmodel accuracy using R² (the square of the correlation coefficient) and root mean squared error \n(RMSE). R² was our primary metric because we wanted to prioritize the model's ability to detect \nincreases in negative emotional states, reflected in corresponding increases in predicted scores. While a \nstronger correlation (and thus higher R²) indicates better alignment between predicted and observed \nvalues, it doesn't necessarily imply lower average prediction error, which is captured by the RMSE. \nInitially, we ran nomothetic models using features extracted from all three NLP approaches, \nincluding all subjects for group-level analysis. These models identified common predictors of negative \nemotional states at the group level. To evaluate how well these group-level models generalized to \nindividual participants, we calculated individual metrics, such as R², derived from the nomothetic \nmodels for each participant. Subsequently, we employed fully idiographic models, building separate \nmodels for each participant to capture person-specific language-emotion associations. These models \nfocused exclusively on within-person variability, aiming for highly individualized predictions. In these \nidiographic models, variables with low variance were removed based on standard frequency criteria, \nwhere the most common value for that variable could not exceed 95% of the total observations. \nFinally, we compared the three abovementioned approaches, to understand their relative strengths in \npredicting negative affect. \nFeature importance   \nTo evaluate feature importance, we used SHAP (SHapley Additive exPlanations) values, which \nquantify the contribution of each feature to the model's predictions in a consistent and interpretable \n 22manner by assessing how variations in a feature impact the model's output 50. SHAP values were \ncalculated using the R packages fastshap and ggbeeswarm , which facilitates visualization and \ninterpretation of feature contributions. This method allowed us to identify the relative importance of \npredictors in the model and their specific effects on the predicted outcomes.  \n \nReferences \n1. Barrett, L. F., Mesquita, B., Ochsner, K. N. & Gross, J. J. The experience of emotion. Annu. Rev. \nPsychol. 58, 373–403 (2007). \n2. Frijda, N. H. The laws of emotion. American Psychologist  43, 349–358 (1988). \n3. Fisher, H. et al. Affect dynamics in adolescent depression: Are all equilibria worth returning to? \n4. Houben, M., Van Den Noortgate, W. & Kuppens, P. The relation between short-term emotion \ndynamics and psychological well-being: A meta-analysis. Psychological bulletin  141, 901–930 \n(2015). \n5. Houben, M. & Kuppens, P. Emotion dynamics and the association with depressive features and \nborderline personality disorder traits: Unique, specific, and prospective relationships. Clinical \nPsychological Science  8, 226–239 (2020). \n6. Kuppens, P. & Verduyn, P. Emotion dynamics. Current Opinion in Psychology  17, 22–26 (2017). \n7. Schoevers, R. et al. Affect fluctuations examined with ecological momentary assessment in \npatients with current or remitted depression and anxiety disorders. Psychological Medicine  51, \n1906–1915 (2021). \n8. Bailen, N. H., Green, L. M. & Thompson, R. J. Understanding emotion in adolescents: A review of \nemotional frequency, intensity, instability, and clarity. Emotion Review  11, 63–73 (2019). \n9. Bennik, E. C., Nederhof, E., Ormel, J. & Oldehinkel, A. J. Anhedonia and depressed mood in \nadolescence: course, stability, and reciprocal relation in the TRAILS study. European child & \nadolescent psychiatry  23, 579–586 (2014). \n 2310. Hollenstein, T. & Lanteigne, D. M. Emotion regulation dynamics in adolescence. in Emotion \nregulation  158–176 (Routledge, 2018). \n11. Davitz, J. R. The Language of Emotion . (Academic Press, 2013). \n12. Pennebaker, J. W., Mehl, M. R. & Niederhoffer, K. G. Psychological aspects of natural language \nuse: Our words, our selves. Annual review of psychology  54, 547–577 (2003). \n13. Jablonka, E., Ginsburg, S. & Dor, D. The co-evolution of language and emotions. Philosophical \nTransactions of the Royal Society B: Biological Sciences  367, 2152–2159 (2012). \n14. Lindquist, K. A. The role of language in emotion: existing evidence and future directions. Current \nopinion in psychology  17, 135–139 (2017). \n15. Iliev, R., Dehghani, M. & Sagi, E. Automated text analysis in psychology: Methods, applications, \nand future developments. Language and cognition  7, 265–290 (2015). \n16. Mihalcea, R. et al. How developments in natural language processing help us in understanding \nhuman behaviour. Nature Human Behaviour  8, 1877–1889 (2024). \n17. Hirschberg, J. & Manning, C. D. Advances in natural language processing. Science 349, 261–266 \n(2015). \n18. Popel, M. et al. Transforming machine translation: a deep learning system reaches news translation \nquality comparable to human professionals. Nature communications  11, 4381 (2020). \n19. Feuerriegel, S. et al. Using natural language processing to analyse text data in behavioural science. \nNature Reviews Psychology  1–16 (2025) doi:10.1038/s44159-024-00392-z. \n20. Carlier, C. et al. In search of state and trait emotion markers in mobile-sensed language: Field \nstudy. JMIR Mental Health  9, e31724 (2022). \n21. Kahn, J. H., Tobin, R. M., Massey, A. E. & Anderson, J. A. Measuring emotional expression with \nthe Linguistic Inquiry and Word Count. The American journal of psychology  120, 263–286 (2007). \n 2422. Sun, J., Schwartz, H. A., Son, Y., Kern, M. L. & Vazire, S. The language of well-being: Tracking \nfluctuations in emotion experience through everyday speech. Journal of Personality and Social \nPsychology  118, 364 (2020). \n23. Boyd, R. L., Ashokkumar, A., Seraj, S. & Pennebaker, J. W. The development and psychometric \nproperties of LIWC-22. Austin, TX: University of Texas at Austin  10, (2022). \n24. Demszky, D. et al. Using large language models in psychology. Nature Reviews Psychology  2, \n688–701 (2023). \n25. Eichstaedt, J. C. et al. Closed-and open-vocabulary approaches to text analysis: A review, \nquantitative comparison, and recommendations. Psychological Methods  26, 398–427 (2021). \n26. Neuendorf, K. A. The Content Analysis Guidebook . (sage, 2017). \n27. Rathje, S. et al. GPT is an effective tool for multilingual psychological text analysis. Proceedings \nof the National Academy of Sciences  121, e2308950121 (2024). \n28. Tausczik, Y. R. & Pennebaker, J. W. The psychological meaning of words: LIWC and \ncomputerized text analysis methods. Journal of language and social psychology  29, 24–54 (2010). \n29. van Loon, A. Three families of automated text analysis. Social Science Research  108, 102798 \n(2022). \n30. Pennebaker, J. W. Linguistic Inquiry and Word Count: LIWC 2001 . (Erlbaum, 2001). \n31. Blei, D. M., Ng, A. Y. & Jordan, M. I. Latent dirichlet allocation. Journal of machine Learning \nresearch 3, 993–1022 (2003). \n32. Griffiths, T. L., Steyvers, M. & Tenenbaum, J. B. Topics in semantic representation. Psychological \nreview 114, 211–244 (2007). \n33. Sivakumar, S. et al. Review on word2vec word embedding neural net. in 282–290 (IEEE, 2020). \n34. Abdurahman, S. et al. Perils and opportunities in using large language models in psychological \nresearch. PNAS nexus  3, pgae245 (2024). \n 2535. Tackman, A. M. et al. Depression, negative emotionality, and self-referential language: A multi-\nlab, multi-measure, and multi-language-task research synthesis. Journal of personality and social \npsychology  116, 817–834 (2019). \n36. Bathina, K. C., Ten Thij, M., Lorenzo-Luaces, L., Rutter, L. A. & Bollen, J. Individuals with \ndepression express more distorted thinking on social media. Nature human behaviour  5, 458–466 \n(2021). \n37. Funkhouser, C. J. et al. Detecting adolescent depression through passive monitoring of linguistic \nmarkers in smartphone communication. Journal of Child Psychology and Psychiatry  65, 932–941 \n(2024). \n38. Beck, E. D. & Jackson, J. J. Personalized prediction of behaviors and experiences: An idiographic \nperson–situation test. Psychological Science  33, 1767–1782 (2022). \n39. Akhtar, M. S., Ghosal, D., Ekbal, A., Bhattacharyya, P. & Kurohashi, S. All-in-one: Emotion, \nsentiment and intensity prediction using a multi-task ensemble framework. IEEE transactions on \naffective computing  13, 285–297 (2019). \n40. Tanana, M. J. et al. How do you feel? Using natural language processing to automatically rate \nemotion in psychotherapy. Behavior research methods  1–14 (2021). \n41. Aalbers, G., Hendrickson, A. T., Vanden Abeele, M. M. & Keijsers, L. Smartphone-Tracked \nDigital Markers of Momentary Subjective Stress in College Students: Idiographic Machine \nLearning Analysis. JMIR mHealth and uHealth  11, e37469 (2023). \n42. Cheung, Y. K. et al. Are nomothetic or ideographic approaches superior in predicting daily \nexercise behaviors? Methods of information in medicine  56, 452–460 (2017). \n43. Rozet, A., Kronish, I. M., Schwartz, J. E. & Davidson, K. W. Using machine learning to derive \njust-in-time and personalized predictors of stress: observational study bridging the gap between \nnomothetic and ideographic approaches. Journal of medical Internet research  21, e12910 (2019). \n 2644. Soyster, P. D., Ashlock, L. & Fisher, A. J. Pooled and person-specific machine learning models for \npredicting future alcohol consumption, craving, and wanting to drink: A demonstration of parallel \nutility. Psychology of Addictive Behaviors  36, 296–306 (2022). \n45. OpenAI et al. Gpt-4 technical report. preprint at arXiv  (2023) doi:10.48550/arXiv.2303.08774. \n46. Gandhi, A., Adhvaryu, K., Poria, S., Cambria, E. & Hussain, A. Multimodal sentiment analysis: A \nsystematic review of history, datasets, multimodal fusion methods, applications, challenges and \nfuture directions. Information Fusion  91, 424–444 (2023). \n47. Hutto, C. & Gilbert, E. Vader: A parsimonious rule-based model for sentiment analysis of social \nmedia text. in vol. 8 216–225 (2014). \n48. Boyd, R. L. & Schwartz, H. A. Natural language analysis and the psychology of verbal behavior: \nThe past, present, and future states of the field. Journal of Language and Social Psychology  40, \n21–41 (2021). \n49. Grün, B. & Hornik, K. topicmodels: An R package for fitting topic models. Journal of statistical \nsoftware 40, 1–30 (2011). \n50. Lundberg, S. A unified approach to interpreting model predictions. Proceedings of the 31st \nInternational Conference on Neural Information Processing Systems  4768–4777 (2017). \n 27Topic \nnumber Topic interpretation  \n1 Social & Evening Activities:  \nfocused on social interactions and \nnighttime activities with friends  friend, talk, sleep, last, night, last_night, see, \ntime, morning, hang, wake, late, talk_friend, \nboyfriend, hang_friend,  \n2 Academic Life:  centered on school-\nrelated activities and academic \nresponsibilities  homework, school, think, class, finish, test, \nnow, take, tomorrow, math, studi, right, \nfinals, essay, stress  \n3 Home activities : capturing leisure \nactivities, particularly around media \nconsumption and family time  watch, nothing, eat, dinner, play, game, show, \nfamily, movie, tv, ate, video, eat_dinner, \nwatch_tv, favorit e, watch_movi e \n4 Family Interactions:  reflecting \nfamily relationships and dynamics mom, think, fun, sister, new, dad, brother, \nroom, read, make, made, book, something, \nfight, clean                 \n5 Daily Activities:  representing \nroutine daily activities like meals  go, went, home, walk, outside, lunch, food, \ndrive, music, back, listen, shop, car, hurt, \naround, dog,  \n6 Personal States & Obligations:  \ndescribing emotional states, needs, \nand work-related responsibilities  get, work, feel, want, today, day, need, good, \nthink, just, felt, like, done, tired , sick,                    Table 1. The 15 most frequent words within the six topics extracted using LDA \n 28 \n  \nSample Characteristics ( N = 97) \n n % \nBiological Sex   \n       Female 75 77.3 \n       Male 22 22.7 \n   \nRace   \n      American Indian  0 0 \n       Asian 13 13.4 \n      Black  10 10.3 \nPacific Islander  1 1.0 \n       White 65 67.0 \n       Multiracial  5 5.2 \n       Other race  3 3.1 \n   \nEthnicity   \n       Hispanic  4 4.1 \n       Not Hispanic  92 94.8 \n       Unknown  1 1.0 \n   \nCurrent Diagnoses (DSM-V)   \n       MDD 21 21.6 \n       GAD 17 17.5 \n       SAD 5 5.2 \n       Panic Disorder  3 3.1 \n       Specific Phobia  2 2.1 \nADHD 1 1.0 \nPTSD 3 3.1 \n   \nMedication   \nPsychotropic Medication  8 8.2 \n   \n M (Range)   SD \nAge (years) 16.2  \n(12 – 18) 1.9 \nFamily Income (dollars) 198,894.8  \n(0 – 500,000) 111,477.5 \n       Table 2. Demographic and Clinical Characteristics of the Sample \n 29 \n \nTable 3. Comparison of Idiographic, Nomothetic, and Nomothetic–Idiographic Model \nPerformance for Negative Affect, Sadness, Anger, and Nervousness Using Random Forest \nand Elastic Net \n \n \n \n \n \n \n \n   Nomothetic \nGroup-Level  \nPerformance  Nomothetic \nPer-Participant Performance  Idiographic \n    R2 R  RMSE R2 R2 range RMSE R2 R2 range RMSE \nNegative \nAffect Random \nForest .38 .62 0.55 .10 \n(.10) .00; .41 \nSig.n= 53/97  0.52 \n(0.22) .10 \n(.12) .00; .47  \nSig.n= 42/97 0.47 \n(0.20) \n  Elastic \nnet \n .17 .41 0.64 .11 \n(.09) .00; .39 \nSig.n= 56/97 0.61 \n(0.27) .10 \n(.12) .00; .53 \nSig.n= 47/97 0.48 \n(0.20) \nSad Random \nForest .33 .58 0.75 .10 \n(.12)  .00; 0.53 \nSig.n= 48/96 0.73 \n(0.27) .10 \n(.13) .00; .46 \nSig.n= 38/94 0.68 \n(0.22) \n  Elastic \nnet \n .14 .37 0.85 .09 \n(.09) .00; .42 \nSig.n= 51/96 0.82 \n(0.32) .10 \n(.11) .00; .51 \nSig.n= 46/94 0.69 \n(0.23) \nAngry Random \nForest .20 .45 .69 .09 \n(.12) .00; .69 \nSig.n= 37/90 0.63 \n(0.33) .06 \n(.07) .00;.33 \nSig.n= 27/85 0.64 \n(0.29) \n  Elastic \nnet \n .13 .36 .72 .07 \n(.07) .00; .45 \nSig.n= 49/90 0.88 \n(0.32) .07 \n(.08) .00; .43 \nSig.n= 39/85 0.66 \n(0.31) \nNervous Random \nForest .24 .49 .85 .06 \n(.08) .00; .44 \nSig.n= 33/95 0.81 \n(0.29) .08 \n(.12) .00; 0.71 \nSig.n= 33/94 0.75 \n(0.27) \n  Elastic \nnet \n .11 .33 .92 .07 \n(.07) .00; .35  \nSig.n= 42/95 0.88 \n(0.33) .09 \n(.12) .00; 0.84 \nSig.n= 43/94 0.76 \n(0.29) \n 30  \n  LIWC+VADER LDA GPT \n    R2 R2 range RMSE R2 R2 range RMSE R2 R2 range RMSE \nNegative \nAffect Random \nForest .05 \n(.08) .00;.42  \nSig.n= \n31/97 0.49 \n(0.21) .07 \n(.09) .00;.48  \nSig.n= \n36/97 0.49 \n(0.21) .10 \n(.08) .00;.34 \nSig.n= \n55/97 0.85 \n(.20) \n  Elastic \nnet \n .07 \n(.10) .00;.55  \nSig.n= \n36/97  0.49 \n(0.22) .06 \n(.07) .00;.44 \nSig.n= \n35/97 0.50 \n(0.22) \nSad Random \nForest .05 \n(.08) .00;.38 \nSig.n= \n24/95 0.70 \n(0.25) .05 \n(.08) .00;.46 \nSig.n= \n25/95 0.71 \n(0.26) .10 \n(.10) .00; .54  \nSig.n= \n55/96 1.11 \n(0.26) \n  Elastic \nnet \n .07 \n(.08) .00;.44  \nSig.n= \n42/95 0.70 \n(0.26) .07 \n(.07)  .00;.29  \nSig.n= \n45/95 0.71 \n(0.27)  \nAngry Random \nForest .04 \n(.06) .00;.28 \nSig.n= \n15/70 0.72 \n(0.29) .03 \n(.05) .00;.58 \nSig.n= \n12/85 0.68 \n(0.32) .09 \n(.10) .00;.44 \nSig.n= \n45/90 0.93 \n(0.34) \n  Elastic \nnet \n .05 \n(.07) .00;.27  \nSig.n= \n26/70 0.73 \n(0.31) .07 \n(.07) .00;.28 \nSig.n= \n43/84 0.67 \n(0.32)   \nNervous Random \nForest .04 \n(0.06) .00;.29 \nSig.n= \n19/86 0.80 \n(0.26) .05 \n(.08) .00;.53 \nSig.n= \n29/94 \n 0.77 \n(0.28) .06 \n(.06) .00;.29 \nSig.n= \n40/95 1.36 \n(0.22) \n  Elastic \nnet \n .05 \n(.07) .00;.35 \nSig.n= \n25/86 .81 \n(28) .05 \n(.05) 00;.23 \nSig.n= \n36/94 \n 0.76 \n(0.28)   Table 4. Comparison of Idiographic Model Performance for Negative Affect, Sadness, Anger, and \nNervousness Using LIWC+VADER, LDA, and GPT \n \n 31 \n \n \n \n  \nFig. 1. The pipeline for processing and analyzing text data from  EMA responses to predict \nemotional states. The process begins with  preprocessing  (e.g., text normalization, stopword \nremoval, stemming). Text features are then extracted  using lexicon-based analysis (LIWC, \nVADER), transformer models (GPT-4), and topic modeling (LDA) . These features, including  GPT \nemotion ratings, linguistic categories, sentiment scores, and topics , are fed into  machine learning \nmodels (Random Forest, Elastic Net)  for prediction. Model performance is evaluated using  R², \nRMSE, and MAE , while SHAP values  provide insight into feature importance. This approach \nintegrates multiple NLP techniques to enhance emotion prediction accuracy.  \n \n 32  \nFig. 2. The relationship between predicted and observed (actual) negative affect ratings \nusing person-specific (idiographic) models. Panel (a) Random Forest (b) Elastic Net. Each \ncolored line represents an individual participant’s predicted estimates (y axis) across \ndifferent levels of actual negative affect (x axis).  \n 33       \nFig. 3. Examples of person-specific (idiographic) predictions of negative affect for \nhigh-performance (top panel) and low-performance (lower panel) models \n  \n 34 \n \n \n \n \n  \n  \nFig. 4. SHAP (Shapley Additive Explanations) beeswarm plots for four individual \nparticipants, illustrating the contribution of different text-based features to the \nmodel’s emotion predictions. Each dot represents an individual data point, with red \nindicating high feature values and blue indicating low feature values. The spread of \ndots across the x-axis reflects the variability in a feature’s effect on predictions across \ndifferent participants. The y-axis lists the features in descending order of importance, \nmeaning the top features had the strongest impact on the model’s predictions. The x-\naxis represents SHAP values, indicating the magnitude and direction of each \nfeature’s impact on the predicted emotion scores. For example, for participant \nK23528, a low score on \"Acquire\" (red color) increases the model’s prediction of \nnegative affect. Conversely, a high feature value for \"Work\" or \"Big Words\" (red \ncolor) contributes to a higher predicted negative affect. \n 35 \n \n Fig. 5. The top panel displays the predictive R² for different models (Random Forest, \nGPT, and Elastic Net) across four emotion categories: negative affect, sadness, anger, \nand nervousness. The bottom panel presents the corresponding RMSE  values. Each \nbox plot represents the distribution of performance metrics for four different NLP-\nbased feature sets: Combined (purple), GPT (yellow), LDA (orange), and \nLIWC+VADER (green) . Higher R² indicates better predictive accuracy, while lower \nRMSE reflects better model fit. \n\n 36 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \na) Negative \n b) Sadness \nc) Anger \n d) Nervousness  \nFig. 6. The pie charts display the distribution of best-performing models for predicting \nnegative emotions (a: Negative, b: Sadness, c: Anger, d: Nervousness) across three comparison \ncategories: nomothetic vs. idiographic models, NLP approaches, and ML models. The colors \nrepresent different model types, indicating the proportion of participants for whom each \nmodel was the best predictor. \n 37Supplement \nSection 1: Latent Dirichlet Allocation (LDA) \nLatent Dirichlet Allocation (LDA) is a probabilistic clustering algorithm that identifies \ntopics in a text corpus by modeling word co-occurrence patterns 31,32. Unlike predefined \ndictionaries, LDA infers topics directly from the data, allowing it to capture latent semantic \nstructures. Each word is assigned probabilistically to one or more topics, iterating until an \noptimal balance between word distributions is reached. This process generates posterior \nprobability distributions that estimate the likelihood of words appearing in specific topics, \nenabling LDA to account for contextual meanings and reduce word sense ambiguity. \nTo improve topic coherence and capture meaningful linguistic patterns, we applied LDA \nto one- to three-word phrases rather than individual words, ensuring that multi-word expressions \n(e.g., \"summer camp\" ) were treated as single units. We implemented LDA separately for \nnomothetic and idiographic analyses. The nomothetic model extracted a uniform set of topics \nacross all participants, while the idiographic model identified personalized topics for each \nindividual. \nFor topic extraction, we used the R package topicmodels  (Grün & Hornik, 2011) and \nperformed algorithmic tuning to determine the optimal number of topics ( k) using ldatuning . The \ntuning process involved evaluating multiple model fit metrics, including Griffiths2004 (Griffiths \net al., 2004), CaoJuan2009 (Cao et al., 2009), and Arun2010 (Arun et al., 2010). These metrics \nprovide different optimization criteria: Griffiths2004 favors maximizing coherence, whereas \nCaoJuan2009 and Arun2010 minimize redundancy and overfitting. Additionally, we considered \nthe Deveaud2014 reference point, which decreases linearly with the number of topics. Based on \nthese criteria, we selected six topics as an optimal balance, as illustrated in Fig. 1S. \n 38Section 2: Prompt GPT \n\"\"\" Evaluate the extent to which the participant experienced each of the following emotions: sadness, anger, \nnervousness, happiness, excitement, interest, boredom, general ratings of positive affect (pa), and general \nratings of negative affect (na) based on their responses to three questions. \n \nWhen evaluating emotions, consider the context of each question. For example, if a participant responds with \n\"none\" to the question about a stressful event, this may indicate lower negative emotions, whereas \nresponding with \"none\" to a question about an enjoyable event may indicate lower positive emotions. \n \nProvide ratings on a scale of 1 to 5 for each emotion and for the general scales, along with a confidence level \nfrom 0 to 100 for each rating. Ensure that the output includes the \"time\" variable from the input. Answer only \nwith ratings and confidence levels; do not include explanations. \n \nParameters: \n \n    participant_id (str): The unique identifier for the participant. \n    time (str): The time point when the responses were recorded. \n    thought (str): The participant's response to what they were thinking just before. \n    stresstext (str): The participant's response to the most stressful event in the last few hours. \n    enjoytext (str): The participant's response to the most enjoyable event in the last few hours. \nReturns: \n \ndict: A dictionary containing the participant's ID, time, their responses, emotion ratings, and confidence \nlevels. \nExample: \n \n        Input: \n            participant_id = \"K23101\" \n            time = \"5\" \n            thought = \"College applications\" \n            stresstext = \"Getting ditched by a friend\" \n            enjoytext = \"Hanging out with my friends at lunch\" \n \n        Output: \n            { \n                'id': \"K23101\", \n                'time': \"5\", \n                'thought': \"College applications\", \n                'stresstext': \"Getting ditched by a friend\", \n                'enjoytext': \"Hanging out with my friends at lunch\", \n                'ratings': { \n                    'Sad': 2, \n                    'Nervous': 1, \n                    'Angry': 2, \n                    'na': 1.67 \n                }, \n                'confidence': { \n                    'Sad': 90, \n                    'Nervous': 70, \n                    'Angry': 75, \n                } \n            }    \"\"\" \n 39Section 3: Nested CV procedure \nThis method consists of two levels of cross-validation: 1. Outer loop: Evaluates the \noverall performance of the model by repeatedly training and testing on different subsets of the \ndata. 2. Inner loop: Conducts model selection and hyperparameter tuning to optimize \nperformance. We used 10-fold cross-validation in both the inner and outer loops. In the outer \nloop, the dataset was divided into ten folds. The model was trained on nine folds and tested on \nthe remaining fold, with this process repeated ten times so that each fold served as the test set \nonce. This iterative process ensures that performance metrics are evaluated across multiple train-\ntest splits, providing a robust estimate of generalizability. Within the inner loop, models were \ntrained on each training fold, and hyperparameter tuning was conducted by selecting the \nparameter combination that maximized performance on a validation set. The best-performing \nmodel was then applied to the corresponding outer loop test set. By separating hyperparameter \noptimization from performance evaluation, this approach prevents data leakage and mitigates the \nrisk of overfitting. Nested cross-validation offers several advantages over a simple train-test split. \nIt allows for efficient use of all available data for both training and validation while reducing the \nbias introduced by a single, potentially unrepresentative data split. Additionally, by incorporating \nmultiple train-test iterations, this approach yields a more robust and reliable estimate of model \nperformance, ensuring that the findings generalize beyond the specific dataset used in training. \nAfter iterating through all outer folds, we aggregated the predictions from the left-out test sets \nand compared them against true values to compute overall predictive performance.    \n 40Section 4: Which Text Features Are Most Predictive in High- vs. Low-Performing Models? \nThough feature importance varied across individuals, we also examined whether specific \npatterns emerged across high- and low-performing participant-specific models. Our goal was to \ndetermine if these patterns could reveal characteristics of participants for whom language is more \nor less predictive of emotional states. Participants were divided into groups based on model \nperformance (R2), with the top 25% (n=25) classified as \"High R2\" and the bottom 25% (n=25) \nas \"Low R2.\" Feature importance scores were analyzed separately for RF and ENR models. \nImportantly, participants with high- and low-performing models showed no significant \ndifference in number of observations (RF: High mean =72.84, Low mean=60.64, t=1.69, p=0.09, \nCI=-2.31;26.71, ENR: High mean =62.72, Low mean=71.36, t=-1.26, p=0.21, CI=-22.38;5.10) or in \nmean negative affect (RF: High mean =0.68, Low mean=0.45, t=1.85, p=0.07, CI=-0.02;0.49, ENR: \nHighmean =0.77, Low mean=0.65, t=0.72, p=0.47, CI=-0.20;0.43). For RF high and low performing \nmodels differed significantly in variability (RF: High mean =0.61, Low mean=0.40, t=3.26, p=0.002, \nCI=0.08;0.33), but not for ENR (High mean =0.59, Low mean=0.50, t=1.29, p=0.20, CI=-0.05;0.22). \nIn addition, as shown in Fig. 5, the top five most important variables for high-performing \nRF models (High R² group) included features from multiple approaches (LIWC, GPT, VADER, \nand LDA). Notably, three of the ten variables that differed the most between the High R² and \nLow R² groups were GPT-derived features (e.g., GPT ratings of negative affect, positive affect, \nand sadness) suggesting that GPT-based variables may have been particularly effective in \nenhancing model performance. In contrast, for high-performing ENR models, the most important \npredictive features were primarily from LIWC (e.g., Authentic, Analytic, Clout), with these \nvariables also exhibiting the largest differences between high- and low-performing models. \nInterestingly, in high-performing ENR models, Authentic  style (associated with perceived \n 41honesty and genuineness) contributed more to predictive accuracy, whereas in low-performing \nmodels, Analytic style  (a metric of logical and formal thinking) played a more prominent role. \nThis suggests that for participants with high-performing models, their emotions may have been \nmore transparently reflected when they used authentic language.  In contrast, participants with \nlow-performing models may have expressed emotions through  analytic language, which tends to \nbe less transparent in conveying affective states. \n \n  \n 42 \n \n Fig. S1. Estimation of the most preferable number of topics for LDA model \n\n 43  \nFig. S2. Distribution of LDA-Derived Topics Across Participants \nParticipants \n 44   Fig. S3. Group differences in feature importance predicting negative affect: random \nforest (RF) and elastic regularization (ENR) \n\n 45 \n \n \n \n \n \n  Fig. S4. Box plots showing the distribution of R² values for matched and mismatched \npredictions across emotion outcomes (NA, Sad, Angry, Nervous) using Elastic Net  \nmodel. \n\n 46 \n \n   \n \n \n \n  Fig. S5. Box plots showing the distribution of RMSE values for matched and \nmismatched predictions across emotion outcomes (NA, Sad, Angry, Nervous) using \nthe Elastic Net model. \n\n 47  \nFig. S6. Box plots showing the distribution of R² values for matched and mismatched \npredictions across emotion outcomes (NA, Sad, Angry, Nervous) using the  Random \nForest model.  \n 48 \nFig. S7. Box plots showing the distribution of RMSE values for matched and \nmismatched predictions across emotion outcomes (NA, Sad, Angry, Nervous) using \nthe Random Forest model."
  },
  "doc-362820e119661525efc80e7426cbea0c": {
    "content": "Page 1: Model Performance Analysis \nUI & Data Controls: \n Title & Layout: The page is set up with a centered, wide layout. The left column contains \ncontrols and dropdowns while the right column displays the visualization and summary table. \n Controls: \no NLP Approach Dropdown: Options include LDA, GPT, COMBINED, and LIWC. \no Idiographic/Nomothetic Dropdown: Changes based on the NLP approach (for GPT, \ndefaults to “Nomothetic” for data availability). \no Outcome Dropdown: Selects the outcome (e.g., Negative Affect, Angry, Nervous, Sad). \nFor GPT, outcome values are standardized (e.g., “negative affect” becomes “na”) and \nfiltered from the file. \nGraph Details: \n Graph Type: Violin Plot \n Axes: \no X-Axis: Represents the “ML Model.” In GPT mode, it shows a single category (“GPT”), \nwhile in other cases it shows two categories: “Elastic Net (en)” and “Random Forest \n(rf).” \no Y-Axis: Represents the R² values, showing the distribution of model performance. \n Data & Filtering: \no GPT: Loads from modelfit_gpt_all.csv filtered by outcome and (if available) the \nnomothetic/idiographic value. \no Non-GPT Models: Loads separate CSV files for Elastic Net and Random Forest; these \ndatasets are concatenated. \n Colors: \no GPT Mode: The violin is rendered in blue. \no Non-GPT: Elastic Net is shown in red, and Random Forest is shown in blue. The \ncolors help differentiate the model types and make it easier to visually compare their \nR² distributions. \n Additional Features: The violin plot includes boxplots and all data points are shown, offering \ninsight into both central tendency and spread. \nSummary Table: \n Content: A table summarizing key statistics per model including the mean (with standard \ndeviation), count (N), range (min to max R²), and the count of p-values below 0.05. \n Calculation: Each metric is computed on the filtered dataset and then presented in a neat \ntabular format. \n \nPage 2: Data Table View – Model Performance per Participant \nUI & Data Controls: \n Layout: The page uses a centered, wide layout with controls in the left column and a data table \nin the right column. \n Controls: \no NLP Approach, Idiographic/Nomothetic, ML Model, and Outcome Dropdowns: These \ndetermine which CSV file is loaded. For GPT, the outcome is chosen from unique \nvalues in the file; otherwise, the outcome is standardized (e.g., “negative affect” to \n“na”). \nData Table: \n Table Type: Interactive Data Table \n Displayed Columns: Typically includes id, participant, r, r2, rmse, and p_value. \n Purpose: Presents participant-level performance metrics from the selected file. \n Axes Explanation: \no Unlike a graph, the table shows rows for each participant and columns for each metric. \n Color and Formatting: \n \n o No explicit colors are used in the table; however, column headers and data formatting \nhelp in quick reading. \n Interaction: The table is scrollable with a fixed height (600px) and width (1200px). \n \nPage 3: True vs Predicted NA Levels \nUI & Data Controls: \n Title & Refresh: The title “True vs Predicted NA Levels” is displayed along with a “Clear All” \nbutton that resets the session state. \n Dynamic Graph Sections: The user can add multiple graph sections; each section has its own \ncontrols for outcome, ML model, and participant selection. \nGraph Details: \n Graph Type: Line Chart \n Axes: \no X-Axis: Represents time (the “time” column from the dataset). \no Y-Axis: Represents NA levels; two lines are plotted —one for the actual (true) NA \nlevels and one for the predicted estimates. \n Data & Filtering: \no Data is loaded from a CSV file whose filename is constructed based on the selected \nML model, outcome, and nomothetic/idiographic value. \no After selecting a participant, the data is filtered to include only rows for that \nparticipant. \n Colors: \no Actual NA Levels: Plotted in teal with a solid line. \no Predicted NA Levels: Plotted in red using a dashed line. \no Correlation Calculation: The correlation coefficient between the actual and predicted \nNA values is computed and displayed in the title. \n Additional Features: \no The graph has fixed dimensions (900 px wide, 400 px high) to ensure consistency, and \ntooltips show detailed information when hovering. \n \nPage 4: Best Model Performance per Participant \nUI & Data Controls: \n Layout: The page features controls on the left (including checkboxes and dropdowns) and a \nresults section on the right. \n Controls: \no Checkbox: “Include both Elastic Net and Random Forest” – when checked, both \nmodels are compared; when unchecked, a dropdown allows selection of one model.  \no Outcome Dropdown: Allows selection of an outcome (e.g., Negative Affect, Angry, \nNervous, Sad). \nData Processing: \n Aggregation: Data from multiple CSV files (covering different NLP approaches and both \nidiographic and nomothetic cases) are merged and filtered. \n Selection: For each participant, the record with the highest R² is selected. \nDisplayed Elements: \n Data Table: \no Content: Shows the best performance for each participant, including Participant, ML \nModel (if both are included), Nomothetic/Idiographic, NLP Approach, R², RMSE, P \nValue, and Counts. \n Pie Charts: \no Graph Types: Pie Charts \no Pie Chart 1 (Nomothetic vs. Idiographic): \n Slices: Represent counts of participants per category. \n \n  Colors: Use Plotly’s default discrete colors to differentiate the groups. \no Pie Chart 2 (NLP Approaches): \n Slices: Represent the frequency of each NLP approach; “comb” is re -labeled as \n“All text features combined.” \n Colors: Also use discrete color mapping. \no Pie Chart 3 (ML Models): \n Shown only if both models are included. \n Slices: Represent the counts for Elastic Net and Random Forest. \n Colors: Defined via a discrete map (Elastic Net in red, Random Forest in blue). \n Axes Explanation for Pie Charts: \no Pie charts do not have traditional axes; the slices’ sizes represent proportions or counts.  \n Legend & Layout: \no The charts are arranged in columns with legends and centered titles for clarity. \n \nPage 5: Feature Importance Heatmap (SHAP Values) \nUI & Data Controls: \n Layout: The page uses a two-column layout with controls in the right column and the heatmap \nvisualization in the left column. \n Controls: \no Outcome and Model Dropdowns: Users select the outcome (e.g., Negative Affect, \nAngry, etc.) and the ML model (Elastic Net (EN) or Random Forest (RF)). \no Participant Multi-select: Allows selection of specific participants, or an “All” option \ncan select every participant. \no Dynamic Symmetric Slider: A slider allows users to set a symmetric threshold for \nfeature importance values. The slider is linked to a callback that forces the two \nhandles to be opposites (e.g., if one is set to –0.005, the other becomes 0.005). \no Info Box: A message box below the slider explains which SHAP importance values \nwill be included (those outside the threshold) and which will be filtered out. \nGraph Details: \n Graph Type: Heatmap \n Axes: \no X-Axis: Represents the selected participants. When “All” is chosen, the x-axis shows a \nsingle label (“All Participants”); otherwise, it shows individual participant names.  \no Y-Axis: Represents the features (variables) that passed the importance threshold. \n Data Processing: \no Data is loaded from a CSV (named by model and outcome). After grouping and \naveraging, the data is pivoted so that features form the rows and participants the \ncolumns. \no The pivoted data is filtered based on the importance threshold. \n Colors: \no Color Scale: A custom color scale is used: \n Deep Blue represents strong negative SHAP values. \n White represents neutral importance. \n Deep Red represents strong positive SHAP values. \no Feature Label Colors: Each feature label on the y-axis is colored according to its NLP \nmethod (using a predefined color map such as LIWC in red, GPT in blue, etc.). \n Layout Adjustments: \no The heatmap’s height and width are dynamically set based on the number of features \nand participants. \no A legend below the graph (built using HTML) explains the color mapping for the NLP \nmethods. \n \nPage 6: Feature Importance per Participant (SHAP Value) \n \n UI & Data Controls: \n Layout: Similar to Page 5, this page uses a two-column layout with controls in the right \ncolumn and the visualization in the left column. \n Controls: \no Outcome & Model Dropdowns: Allow the user to select the outcome and ML model \n(with outcome normalized). \no Participant Dropdown: Enables the selection of a single participant from the loaded \ndataset. \no Performance Metrics Loading: A secondary CSV is loaded to extract performance \nmetrics (R² and RMSE) for the selected participant, which are later shown in the graph \ntitle. \no Dynamic Symmetric Slider: As on Page 5, a slider is provided for setting the threshold \nfor feature importance. It uses a symmetric range and displays an info box. \nGraph Details: \n Graph Type: SHAP Summary Scatter Plot with Vertical Lines \n Axes: \no X-Axis: Represents the SHAP (feature importance) values. \no Y-Axis: Represents features. The y-axis values are evenly spaced for each feature, and \nfeature names are used as tick labels. \n Data Processing: \no Data is filtered for the selected participant and then filtered based on the chosen \nimportance threshold. \no The data is sorted by NLP method (and importance) and assigned evenly spaced y-axis \npositions. \n Colors: \no Vertical Lines & Markers: Each feature’s importance is shown with a line extending \nfrom 0 to the SHAP value, with a dot at the end. The color of the line and dot \ncorresponds to the feature’s NLP method (e.g., LIWC is red, GPT is blue, etc.). \no Legend: Additional dummy traces ensure every NLP method appears in the legend. \n Additional Features: \no The plot title includes the participant’s name along with their performance metrics (R² \nand RMSE). \no A dashed vertical line at x=0 helps delineate positive from negative importance. \n \nPage 7: Feature Importance Analysis \nUI & Data Controls: \n Layout: The page is divided into a left column for controls and a right column for the graphs. \n Controls: \no Dropdowns: \n ML Model & Outcome: Users select the model and outcome (with outcome \nstandardized). \no Checkbox: “Include the variable 'Time'” (default is checked). \no Participant Filtering: A slider lets users select the percentage of participants in each \ngroup based on performance (R²). \no Minimum Variable Occurrence Slider: Sets a threshold for how many participants \nmust have a feature for it to be included. \n Data Processing: \no Performance Data: Loaded from a performance CSV, participants are split into high \nand low R² groups. \no Feature Importance Data: Loaded from another CSV, with an option to exclude the \n“Time” variable. \no Aggregation: The absolute mean importance for each feature is computed separately \nfor high and low R² groups. The absolute mean difference is also calculated. \n \n Graph Details: \n Graph Type: Bar Charts (two separate charts) \n First Bar Chart – High vs. Low R² Groups: \no Axes: \n X-Axis: Represents the absolute mean value for each group. \n Y-Axis: Lists the features (sorted by the high R² group’s values) with labels \ncolor-coded by NLP method. \no Bars: \n Two sets of bars for each feature: one for the high R² group (colored red) and \none for the low R² group (colored turquoise). \n Second Bar Chart – Absolute Mean Difference: \no Axes: \n X-Axis: Represents the absolute mean difference between the high and low R² \ngroups. \n Y-Axis: Lists the top features (sorted by difference) with color-coded labels. \no Bars: \n A single set of gray bars represents the difference. \n Colors: \no The bar colors (red, turquoise, and gray) are chosen to clearly contrast the groups, and \nthe y-axis labels are enhanced with HTML to include NLP method colors. \nLegend: \n An HTML-based legend at the bottom explains the NLP method color coding. \n \nPage 8: Common Top Predictive Features \nUI & Data Controls: \n Layout: This page uses a sidebar layout. \no Left Sidebar (Controls): \n Model Selection Dropdown: Allows the choice between Elastic Net (EN) and \nRandom Forest (RF). \n Sliders: \n Number of Features per Participant (determines how many top \nfeatures per participant are selected). \n Number of Variables in Figure (sets how many variables appear in the \naggregate view). \n SHAP Value Threshold (filters features based on a minimum absolute \nSHAP value). \n Checkbox: “Use ABS values only” toggles the data source and indicates that \nonly absolute values are considered. \n Data Processing: \no For each emotion (na, sad, angry, nervous), a CSV file is loaded based on the selected \nmodel and whether ABS values are used. \no Each participant’s top features (based on the SHAP threshold) are aggregated. The \ncount of participants selecting each feature is calculated, and the SHAP sign (Positive \nor Negative) is determined. \n Graph Details: \no Graph Type: Stacked Horizontal Bar Charts \no Axes: \n X-Axis: Represents the “count” (or percentage of participants) in which the \nfeature is selected. \n Y-Axis: Represents the feature names, with labels color-coded based on their \nassociated NLP method. \no Bar Colors: \n Bars are split by SHAP sign: \n \n  Positive Values: Shown in a teal-like color (rgb(0,182,185)). \n Negative Values: Shown in a red hue (rgb(255,79,82)). \n Layout of Graphs: \no The four emotion-specific bar charts are arranged in a 2×2 grid in the main area. \n Legend: \no A legend below the charts (rendered in HTML) explains the NLP method colors."
  },
  "doc-54805caf2c07ba0a1edb9fdf59ae4049": {
    "content": "LIWC variables: \nSummary Variables:  \n Word Count (WC):  Total number of words in the text. \n Analytical Thinking (Analytic):  Assesses the degree of logical and formal thinking in \nthe text. Higher scores indicate more analytical thinking. \n Clout (Clout):  Measures the author's perceived confidence or leadership. Higher scores \nsuggest a more authoritative tone. \n Authenticity (Authentic):  Reflects honesty and genuineness in the text. Higher scores \nindicate a more candid and personal tone. \n Emotional Tone (Tone):  Evaluates the overall emotional positivity or negativity. Scores \nabove 50 suggest a more positive tone, while scores below 50 indicate a more negative \ntone. \nLinguistic Dimensions:  \n Words per Sentence (WPS):  Average number of words per sentence, indicating \nsentence complexity. \n Big Words (BigWords):  Percentage of words with seven or more letters, reflecting the \nuse of complex vocabulary. \n Dictionary Words (Dic):  Percentage of words captured by the LIWC dictionary, \nindicating the proportion of recognized words. \n Total Function Words (function):  Percentage of function words (e.g., articles, \nprepositions) used, which are essential for grammatical structure. \n Total Pronouns (pronoun):  Percentage of pronouns, reflecting the focus on people or \nobjects. \n Personal Pronouns (ppron):  Percentage of personal pronouns (e.g., I, you, we), \nindicating a focus on individuals. \no 1st Person Singular (i):  Use of first-person singular pronouns (I, me, my), \nsuggesting a personal focus. \no 1st Person Plural (we):  Use of first-person plural pronouns (we, our, us), \nindicating a collective focus. \no 2nd Person (you):  Use of second-person pronouns (you, your), addressing the \nreader directly. \no 3rd Person Singular (shehe):  Use of third-person singular pronouns (he, she, \nhim, her), referring to other individuals. \no 3rd Person Plural (they):  Use of third-person plural pronouns (they, their, them), \nreferring to groups of others. \n Impersonal Pronouns (ipron):  Use of impersonal pronouns (it, that, anything), often \nindicating abstract or unspecified subjects. \n Determiners (det):  Use of words that introduce nouns (this, that, my), specifying \nparticular items. \n Articles (article):  Use of articles (a, an, the), which precede nouns to define them as \nspecific or unspecific. \n Numbers (number):  Use of numerical terms (one, two, first), indicating quantification. \n Prepositions (prep):  Use of prepositions (to, of, in), which show relationships between \nwords. \n Auxiliary Verbs (auxverb):  Use of auxiliary verbs (is, was, be), which accompany main \nverbs to express tense, mood, or voice. \n Adverbs (adverb):  Use of adverbs (so, just, very), which modify verbs, adjectives, or \nother adverbs. \n Conjunctions (conj):  Use of conjunctions (and, but, because), which connect clauses or \nsentences. \n Negations (negate):  Use of negation words (no, not, never), indicating denial or \ncontradiction. \n Common Verbs (verb):  Use of frequently occurring verbs (e.g., be, have, do), indicating \nactions or states. \n Common Adjectives (adj):  Use of frequently occurring adjectives (e.g., good, new, \nhigh), describing qualities or states. \n Quantities (quantity):  Use of quantifiers (e.g., all, many, some), indicating amounts or \nproportions. \nPsychological Processes:  \n Affect (Affect):  Use of emotion-related words, indicating the expression of feelings.  \no Positive Tone (tone_pos):  Use of positive emotion words (e.g., happy, love), \nreflecting positive sentiment. \no Negative Tone (tone_neg):  Use of negative emotion words (e.g., sad, hate), \nreflecting negative sentiment. \no Anxiety (emo_anx):  Use of words related to anxiety (e.g., worried, nervous), \nindicating anxious feelings. \no Anger (emo_anger):  Use of words related to anger (e.g., mad, furious), \nindicating angry feelings. \no Sadness (emo_sad):  Use of words related to sadness (e.g., crying, grief), \nindicating sorrowful feelings. \n Cognitive Processes (cogproc):  Use of words related to thinking processes, indicating \ncognitive activity. \no Insight (insight):  Use of words indicating understanding (e.g., know, realize), \nreflecting insightfulness. \no Causation (cause):  Use of words indicating causality (e.g., because, effect), \nreflecting causal reasoning. \no Discrepancy (discrep):  Use of words indicating differences (e.g., should, would), \nreflecting discrepancy. \no Tentative (tentat):  Use of tentative words (e.g., maybe, perhaps), indicating \nuncertainty."
  }
}