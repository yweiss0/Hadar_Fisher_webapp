{
  "chunk-33ad2b7f24e34710b479f04714457afa": {
    "tokens": 1200,
    "content": "1 Can Natural Language Processing Effectively Track Negative Emotions in the Daily Li ves \nof Adolescents ? \nEmotions play a fundamental role in human life, serving as essential cues that influence our \nactions and interactions with the environment (Barrett et al., 2007) . Emotional states act as \nimmediate alerts to potential benefits or dangers, driving us towards actions that align with our \npersonal goals and away from potential threats (Frijda, 1988) . Their dynamic nature, \ncharacterized by  frequent  shifts over time, significantly impacts mental health. Research shows \nthat both the intensity of emotions and their temporal dynamics can contribute to the \ndevelopment of mood disorders (Fisher et al., n.d.; Houben et al., 2015; Houben & Kuppens, \n2020; Kuppens & Verduyn, 2017; Schoevers et al., 2021) . This is especially important for \nadolescents —a developmental period marked by high negative emotions and rapid mood \nfluctuations , all of which contribute to their elevated risk for mood disorders such as depression \nand anxiety  (Bailen et al., 2019; Bennik et al., 2014; Hollenstein & Lanteigne, 2018) . Given this \nprofound impact, developing effective methods to measure daily emotional changes that are \nspecifically suitable for adolescents is crucial . These measurements could identify both acute \nemotional distress and long er-term maladaptive patterns, serving two key purposes: enabling \ntimely interventions during periods of high negative affect and facilitating targeted intervention \nto address maladaptive emotional patterns. Such methods could  help predict  increased  risk of  \nmood disorder onset, tailor tr eatment plans, and evaluate intervention efficacy, ultimately \ncontributing to improved mental health outcomes.   \nLanguage offers a promising avenue for measuring emotion, as people convey their \nemotional states both explicitly and implicitly through their choice of words and their manner of \nspeaking or writing (Davitz, 2013; Pennebaker et al., 2003) . Language plays a dual role in the \nrealm of emotions, serving not only as a primary tool for expressing and communicating \n 2 emotional states but also as an actively shaping how emotions are experienced (Jablonka et al., \n2012; Lindquist, 2017) . Therefore, the current study aims to explore whether text can be used to \ntrack within -person fluctuation in emotional states.  \nWith the rise of digital communication, people , especially adolescents , now express \nthemselves extensively through text -based interactions, including social media posts, messages, \nand online discussions. This shift has generated an unprecedented volume of language data, \noffering a unique opportunity to study adolescent emotion s on a larger scale (Iliev et al., 2015) . \nAdvances in natural language processing (NLP), a multidisciplinary field combining computer \nscience, artificial intelligence, and linguistics, have enabled researchers to analyze this data \nefficiently and meaningfully  (Hirschberg & Manning, 2015; Manning, 1999) . One key advantage \nof NLP over traditional, manual text analysis is its ability to handle large datasets —such as \nthousands of social media posts or digitized texts —quickly and efficiently. By leveraging these \nadvancements, NLP methods can identify patterns in language use  (e.g.,  sentiment , tone, and \nself-referential quality) , offering real -time insights into emotional states without requiring \nparticipants to actively report their feelings (Carlier et al., 2022; Kahn et al., 2007; Sun et al., \n2020) . Importantly, text -based communication provides insights not only into the intensity of \nemotions but also into the context in which those emotions arise. This contextual information can \nreveal situational factors contributing to emotional experiences, off ering a more nuanced \nunderstanding of emotional processes. While NLP has been used in behavioral science for \ndecades, its increased accessibility and affordability have significantly improved, making it an \nincreasingly popular tool (Feuerriegel et al., 2025) . \nIn recent years, various text analysis tools have emerged (Boyd et al., 2022; Demszky et \nal., 2023; Eichstaedt et al., 2021; Neuendorf, 2017; Rathje et al., 2024; Tausczik & Pennebaker, \n 3 2010; van Loon, 2022) , ranging from basic approaches like counting word frequencies to more \nadvanced methods, such as leveraging large language models (LLMs), each with its own set of \nadvantages and limitations. For example, closed -vocabulary programs such as LIWC \n(Pennebaker, 2001) , use predefined dictionaries to categorize words. While highly interpretable, \ntransparent, and efficient at summarizing concepts, these methods often neglect context, leading \nto potential misinterpretations (Eichstaedt et al., 2021) . In contrast, open -vocabulary approaches, \nsuch as Latent Dirichlet Allocation (LDA), and word embedding methods, leverage data -driven \ntechniques to examine a broader spectrum of words and topics (Blei et al., 2003; Griffiths et al., \n2007; Sivakumar et al., 2020) . These methods are better at capturing nuances, addressing \nambiguous word meanings, and are less susceptible to misinterpretations. Limitations of open -\nvocabulary approaches include the need for more technical expertise, larger datasets, and careful \nconsi deration of parameter choices, as well as challenges in interpretability  (Eichstaedt et al",
    "chunk_order_index": 0,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-921a7a6310245e5d30b940d360ac60ef": {
    "tokens": 1200,
    "content": "topics (Blei et al., 2003; Griffiths et al., \n2007; Sivakumar et al., 2020) . These methods are better at capturing nuances, addressing \nambiguous word meanings, and are less susceptible to misinterpretations. Limitations of open -\nvocabulary approaches include the need for more technical expertise, larger datasets, and careful \nconsi deration of parameter choices, as well as challenges in interpretability  (Eichstaedt et al., \n2021; van Loon, 2022) . More recently, LLMs like GPT have shown great promise in accurately \nidentifying various psychological constructs in text, overcoming many of the constraints of older \nmethods (Rathje et al., 2024). These models can interpret the context of words and have achieved \neffective results across multiple languages with simple prompts. The m ain limitations of this \napproach include a lack of transparency in how inferences were generated and difficulties in \nreproducing results due to their probabilistic nature (Abdurahman et al., 2024) . For example, \nalthough LLM -based approaches could predict whether  a person experience s negative emotion s \nbased on  social media posts, LLMs could not  be used to identify which linguistic cues (such as \nthe use of pronouns)  predict this experience  (Feuerriegel et al., 2025; Rathje et al., 2024) . In the \nlatter case, dictionary -based methods  are preferrable. Ultimately, closed - and open -vocabulary \napproaches, along with advancements in LLMs, provide complementary strengths that \n 4 significantly enhance our ability to understand psychological states through language. By \ncombining these methods, researchers can leverage their unique advantages while mitigating \ntheir limitations, offering a robust alternative to traditional  and relatively cumbersome  self-report \nmeasures  assessing emotions . \nDespite the increasing sophistication of these methods, significant gaps remain in the \ncurrent literature. Most importantly, existing studies on text -based emotion prediction have \nfocused on exploring between -person differences, primarily identifying which  text features are \nassociated with individuals experiencing high levels of negative emotions  (e.g., Tackman et al., \n2019) . These approaches aggregate data across participants and aim to pinpoint common \nlinguistic markers, such as the use of first -person pronouns or negative sentiment, that are \nindicative of heightened emotional distress and depression  (Bathina et al., 2021; Funkhouser et \nal., 2024) . However, this method overlooks both between -person differences in how linguistic \nmarkers relate to emotions and within -person fluctuations in emotional states . Even when using \nmultilevel models to predict within -person fluctuations  in emotions , models assumes that the \nsame linguistic features signal high levels of negative emotions across different individuals \n(Funkhouser et al., 2024) . Such assumptions fail to account for the fact that language is \ninherently idiosyncratic, with individuals expressing emotions in unique and context -dependent \nways. Beck & Jackson, (2022)  demonstrated the importance of idiographic approaches by \nshowing that the psychological and situational antecedents that predicted future loneliness varied \nsubstantially across participants, with no two individuals showing the same pattern of predictive \nfeatures. This highlights the need for personalized models that adapt to the distinctive ways \npeople express emotions through text, enabling more accurate predictions tailored to each \nindividual .  \n 5 Moreover, previous research has typically studied language features in isolation, missing \nthe potential benefits of combining different language -based methods (Carlier et al., 2022) . This \nis especially important as features that may be significant for one individual may be not \nimportant for others. To address these limitations, t his study integrat es diverse language -based \ntools and leverag es machine learning to develop personalized  models capable of monitoring \nwithin -person fluctuations in emotional states.  \nThe Present Study  \nThe study's primary goal is to evaluate the effectiveness of various text analysis and \nmachine learning techniques in tracking within -person fluctuations in negative affect (NA) over \ntime.  Since  individuals express emotions in a variety of linguistic ways, this research aims to \ndevelop more personalized approaches to emotion tracking.  \nIn addition to this main objective , we address two key questions:  \n1) Do idiographic  (individ ual-level ) models outperform nomothetic  (group -level)  models \nin predicting fluctuations?  This question is motivated by  the fact that  individuals  may \nexpress emotions through language in highly unique ways.  \n2) Is combining various NLP approaches beneficial for emotion prediction?  \nThe research employs and compares three types of Natural Language Processing (NLP) \napproaches , which, as noted above, have complementary strengths and limitations : \na) Closed vocabulary (LIWC and VADER)  \nb) Open vocabulary (LDA)  \nc) Large Language Models (GPT)  \nBy addressing these questions, the study aims to enhance the accuracy of emotion \nprediction models, potentially enabling closer monitoring of emotional fluctuations in daily life. \n 6 The ultimate goal is to use these improved emotion predictions to inform the delivery of scalable, \nreal-time, and personalized interventions for alleviating high NA states and improving emotion \nregulation abilities.  \nMethod  \nParticipants  \nParticipants were 98 English -speaking adolescents aged 12 -18 (XX female, XX male; mage \n= XX, SD = XX) recruited from the greater Boston area , derived from two larger studies that \nrecruited typically developing (non -anhedonic) adolescents, as well as adolescents with elevated \nlevels of anhedonia. Exclusion criteria included history or current diagnosis of any of the \nfollowing DSM -",
    "chunk_order_index": 1,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-735b2618dc55b681bb258895b56ec379": {
    "tokens": 1200,
    "content": "and improving emotion \nregulation abilities.  \nMethod  \nParticipants  \nParticipants were 98 English -speaking adolescents aged 12 -18 (XX female, XX male; mage \n= XX, SD = XX) recruited from the greater Boston area , derived from two larger studies that \nrecruited typically developing (non -anhedonic) adolescents, as well as adolescents with elevated \nlevels of anhedonia. Exclusion criteria included history or current diagnosis of any of the \nfollowing DSM -5 psychiatric illnesses: major depressive disorder, schizophrenia spectrum or \nother psychotic disorder, bipolar disorder, substance or alcohol use disorder within the past 12 \nmonths or lifetime sever e substance or alcohol use disorder , as well as current diagnosis of \nanorexia nervosa or bulimia nervosa  for an additional information about sample inclusion and \nexclusion criteria see Murray et al., (2023) . \nProcedure    \nAll procedures were approved by the Mass General Brigham IRB. Written informed \nconsent was provided by participants who were 18 years of age, as well as from parents of \nparticipants who were under 18 years of age, along with the participant’s written informed \nassent. At the baseline session participants were administered a semi -structured clinical \ninterview , the Kiddie Schedule for Affective Disorders and Schizophrenia (K -SADS; Kaufman \net al., 1997) , either in -person or over Zoom . Participants also  completed self -report measures  and \ninstalled the MetricWire App on their smartphones to complete ecological mom entary \nassessments (EMAs). EMA surveys were delivered 3-4 times per day whereby participants were \n 7 randomly signaled during two timeslots ( 4 pm  to 6:30 pm  and 6:30 pm  to 9 pm ) during a 5 -day \nperiod (Thursday - Monday). A third survey was sent on weekends ( 11 am -4 pm ). Of the \nparticipants, 39 completed the EMA for four consecutive weeks, while 59 completed it over four \nweeks on an every -other -week schedule.  \nEcological Momentary Assessment (EMA): Participants were asked to rate on a 5 -point \nLikert scale, ranging from 1, “Very slightly or not at all,” to 5, “Extremely”  the extent to which \nthey were feeling several emotions immediately before they started the assessment . Participants’ \nnegative affect (NA) included responses for: “sad,” “nervous,” and “angry.” Mean NA was \nmeasured by averaging the three NA variables.  In addition, participants responded to the \nfollowing open -ended questions: 1) What were you thinking about right before you started this \nsurvey?  2) Think about the most enjoyable or happy time  since you completed the last survey  (or \nif this is your first survey, then the last 24 hours). Very briefly, what happened (1 -2 sentences is \nfine)?  3) Think about the most stressful or negative time since you completed the last survey  (or \nif this is your first survey, then the last 24 hours). Very briefly, what happened (1 -2 sentences is \nfine)?  Participants with fewer than 30 observations were excluded from the analysis.  \nLanguage Measures  \nWe used the following strategies to extract text features and generate quantitative \nsummaries of the language data.  \nClose vocabulary : For close vocabulary, we extracted features using the Linguistic \nInquiry and Word Count (LIWC ; Tausczik & Pennebaker, 2010)  and the Valence Aware \nDictionary and sEntiment Reasoner (VADER ; Hutto & Gilbert, 2014) . LIWC is a computerized \ntext analysis tool that categorizes words into over 90 linguistic and psychological dimensions \nbased on an internal dictionary of approximately 6,400 words  (Boyd & Schwartz, 2021) . It \n 8 calculates the percentage of words that match each predefined category, offering insights into \nlinguistic structures (e.g., pronouns), psychological constructs (e.g., affect), and broader \nlanguage patterns (e.g., analytical thinking).  The LIWC has been extensively validated across \nnumerous studies and is widely used in psychological research to quantify language use patterns \nassociated with various psychological states and traits. For this study, we used LIWC -22 (Boyd \net al., 2022) , the latest version of the software, the analyze participants'  responses to the EMA \nopen questions.  \nVADER  is a simple rule -based model for general sentiment analysis  optimized for social \nmedia . It provides four sentiment scores: negative, positive, neutral, and compound (an overall \nsentiment score from -1 to +1). VADER is particularly effective at handling sentiments \nexpressed in short, informal text and accounts for factors like punctuation, capita lization, and \nmodifiers (e.g., intensifiers like \"very\") that influence the intensity of the sentiment.  Whereas \nLIWC excels in offering detailed psychological and linguistic insights across a wide range of \ntexts, VADER is more attuned to detecting sentiment polarity and intensity in social media. \nFinally , text lengths were extracted as features for each question and included in the models.  \nOpen vocabulary : Latent Dirichlet Allocation (LDA) is a probabilistic clustering method \nthat groups words into topics based on their co -occurrence across a text corpus  (Blei et al., 2003; \nGriffiths et al., 2007) . Unlike predefined dictionaries, LDA generates topics directly from the \ndata, identifying latent patterns in word usage. Each word is assigned to one or more topics, \niterating until an",
    "chunk_order_index": 2,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-4207b53ecda877484dc8806d5c2897ac": {
    "tokens": 1200,
    "content": "the models.  \nOpen vocabulary : Latent Dirichlet Allocation (LDA) is a probabilistic clustering method \nthat groups words into topics based on their co -occurrence across a text corpus  (Blei et al., 2003; \nGriffiths et al., 2007) . Unlike predefined dictionaries, LDA generates topics directly from the \ndata, identifying latent patterns in word usage. Each word is assigned to one or more topics, \niterating until an optimal balance is reached. This results in a set of posterior probability \ndistributions, which approximates the likelihood of each word occurring within each topic. This \nallows LDA to create semantically coherent clusters, overcoming word sense ambiguities by \ncontextually assigning words to topics. We ran LDA on one - to thr ee-word phrases, rather than \n 9 individual words alone, so that phrases such as  ‘Summer camp ’ are treated as a single term . We \nimplemented Latent Dirichlet Allocation (LDA) using distinct approaches for nomothetic and \nidiographic analyses. For the nomothetic model, we extracted a uniform set of topics across the \nentire sample. For the idiographic model, we derived personalized topics unique to each \nindividual.  \nThe latent topics were extracted from the preprocessed text corpus using probabilistic \nmodeling with the R package topicmodels (Grün & Hornik, 2011) . For the whole -sample \n(nomothetic) LDA, before creating the model, we performed the tuning of the algorithm to \ndefine the number k of topics using ldatuning . To determine the optimal number of topics, we \nevaluated the quality of topic modeling using three metrics: Griffiths2004 (Griffiths2004), \nCaoJuan2009  (Cao et al., 2009) , and Arun2010 (Arun et al., 2010) . These metrics included both \nminimization criteria (CaoJuan2009 and Arun2010), which are optimized when minimized, and \nmaximization criteria (Griffiths2004), which are optimized when maximized. Additionally, \nDeveaud2014  provided a reference point that decreases linearly with the number of topics.  We \nselected six topics based on model fit metrics, as illustrated in Figure 1. The selection of six \ntopics reflected a balance among these metrics. Table 1 represents  each topic by its 15 most \nhighly frequent  words, providing a clear semantic foundation for further analysis.  \nLarge Language Models (GPT).  To generate emotion ratings using large language \nmodels (LLMs), we employed the Generative Pre -trained Transformer (GPT), an autoregressive \nAI language model developed by OpenAI. GPT is built on a transformer -based architecture, a \nneural network design tha t excels at processing sequential data by using self -attention \nmechanisms to capture contextual relationships across words. This allows GPT to generate \nhuman -like text by predicting the next word in a sequence based on the prov ided context.  \n 10 Specifically, we utilized GPT -4, an advanced version pre -trained on a vast dataset \n(45TB), enabling it to generate coherent sentences and perform various tasks such as writing, \nanswering questions, and engaging in conversations. In this study, we prompted GPT-4 to rate \nthe extent to which a participant experienced one of the following emotions: Sadness, Anger, and \nNervousness, on a scale of 1 to 5 (similar to the scale used in EMA), based on responses to three \nopen -text questions. See the online supplement for the full prompt used to generate GPT \nresponses.  \nData Analysis  \nModel Specification  \nWe compared both nomothetic (group -level) and idiographic (individual -level) models in \ntheir ability to predict variability in negative emotional states within  individuals over time.  We \nemployed two machine learning approaches to predict negative affect: elastic net regression and \nrandom forest models. Elastic net regularization (ENR) is a popular regression technique that \ncombines two types of penalties: ridge and lasso. This combina tion helps address issues related \nto multicollinearity by constraining the coefficients of correlated variables while also minimizing \nmodel overfitting. On the other hand, random forest (RF) is an ensemble learning method based \non decision trees. Unlike EN R, random forest can capture complex nonlinear relationships and \ninteractions between variables without having to specify them in advance. Given that these two \nmethods use different strategies for selecting and weighting variables, comparing them can help \nto determine which one provides the most accurate predictions in a given context.  \nTo assess model performance and generalizability while minimizing overfitting, we \nimplemented a nested cross -validation (CV) procedure using the nestedcv  package (Lewis et al., \n2023). This method consists of two levels of cross -validation:  1. Outer loop: Evaluates the \n 11 overall performance of the model by repeatedly training and testing on different subsets of the \ndata.  2. Inner loop: Conducts model selection and hyperparameter tuning to optimize \nperformance.  We used 10 -fold cross -validation in both the inner and outer loops. In the outer \nloop, the dataset was divided into ten folds . The model was trained on nine folds and tested on \nthe remaining fold, with this process repeated ten times so that each fold served as the test set \nonce. This iterative process ensures that pe rformance metrics are evaluated across multiple train -\ntest splits, providing a robust estimate of generalizability.  Within the inner loop, models were \ntrained on each training fold, and hyperparameter tuning was conducted by selecting the \nparameter combination that maximized",
    "chunk_order_index": 3,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-bfff40ad3e467fa5b72569303f6d0250": {
    "tokens": 1200,
    "content": "loop, the dataset was divided into ten folds . The model was trained on nine folds and tested on \nthe remaining fold, with this process repeated ten times so that each fold served as the test set \nonce. This iterative process ensures that pe rformance metrics are evaluated across multiple train -\ntest splits, providing a robust estimate of generalizability.  Within the inner loop, models were \ntrained on each training fold, and hyperparameter tuning was conducted by selecting the \nparameter combination that maximized performance on a validation set. The best -performing \nmodel was then applied to the corresponding  outer loop test set. By separating hyperparameter \noptimization from performance evaluation, this approach prevents data leakage and mitigat es the \nrisk of overfitting.  Nested cross -validation offers several advantages over a simple train -test split. \nIt allows for efficient use of all available data for both training and validation while reducing the \nbias introduced by a single, potentially unrepresentative data split. Ad ditionally, by incorporating \nmultiple train -test iterations, this approach yields a more robust and reliable estimate of model \nperformance, ensuring that the findings generalize beyond the specific dataset used in training.  \nAfter iterating through all outer folds, we aggregated the predictions from the left -out test \nsets and compared them against true values to compute overall predictive performance. We \nassessed model accuracy using R²  and root mean squared error (RMSE).  We consider RMSE to \nbe an intuitive metric for assessing predictive accuracy on a target variable measured on a 7 -\npoint Likert scale, as it accounts for larger errors more heavily and allows us to make statements \nsuch as, “on average, the model’s predictio n deviates from momentary subjective stress by \napproximately 0.80 points on a 7 -point scale.”  \n 12  \nFirst, we ran nomothetic models that included features extracted from all three NLP \napproaches, including all subjects  for group -level analysis. These models aimed to identify \ncommon predictors of negative emotional states at the group level.  To assess how well group -\nlevel models performed for individual participants , we adopted a nomothetic -idiographic \napproach, where metrics derived from nomothetic models  such as R2 were calculated separately \nfor each participant.  \nFinally, we employed fully idiographic models, building separate models for each \nparticipant to capture person -specific language -emotion associations . These models focus \nexclusively on within -person variability, enabling highly individualized predictions. By \ncomparing the three  abovementioned  approaches, we aimed to understand their relative strengths \nin predicting negative affect.  In these  idiographic models, variables with low variance were \nremoved based on standard frequency criteria, where the most comm on value  for that variable  \ncould not exceed 95% of the total observations.  \n \nFeature importance  \nTo evaluate feature importance, we used SHAP (SHapley Additive exPlanations) values, \nwhich quantify the contribution of each feature to the model's predictions in a consistent and \ninterpretable manner by assessing how variations in a feature impact the model's output  \n(Lundberg, 2017) . SHAP values were calculated using the R package s fastshap  and ggbeeswarm , \nwhich facilitates visualization and interpretation of feature contributions. This method allowed us \nto identify the relative importance of predictors in the model and their specific effects on the \npredicted outcomes.  \n 13 Results  \nDemographic and clinical characteristics  \nTable 2 presents the demographic and clinical characteristics of the participants. \nParticipants  completed an average of XX EMA surveys (s.d. =  XX). Average EMA compliance \nwas XX% (s.d. =  XX%).  \nModel performance  \nModel performance metrics for the nomothetic (one model combinging all  observations \nfrom all participants), nomothetic -idiographic ( same as above  but calculating performance \nseparately for each participant), and idiographic (building separate  model s for each individual) \napproaches using text features extrac ted from the three NLP approaches to predict negative \naffect are shown in Table 3.  \nThe nomothetic model s generally outperformed both the nomothetic -idiographic and \nidiographic models across all emotions. Specifically, when predicting negative affect, sadness, \nanger, and nervousness, the nomothetic RF model yielded R² values of 0.38, 0.33, 0.20 and 0.24, \nrespectively, whereas the nomothetic ENR model yielded R² values of 0. 17, 0.14, 0.13 and 0. 11, \nrespectively . The substantially higher performance of the random forest model can be attributed \nto its ability to account for participant ID  (grouping by individuals) , which Elastic net cannot. \nThis inference is supported by the feature importance analysis shown in Figure 2, where \nparticipant ID emerges as the most important feature in the RF model. This finding points to \nsubstantial differences between individuals in their average levels of nega tive affect , \nemphasizing the importance of person -specific modeling approaches to better capture within -\nperson fluctuations.   \n 14 Building on this and aligning with the ultimate translational goal of using text to track \nwithin -person fluctuations in negative emotions (rather than between -person differences), we \nproceed to evaluate the model's predictive performance for each participant individually \n(nomothetic -idiographic approach). Performance decreased substantially,  with mean R² values in \nthe range of 0.06 -0.11.  However, there was considerable variability in predictive accuracy across \nparticipants. For example, when predicting negative affect , R² values ranged from 0.00 to 0.41.",
    "chunk_order_index": 4,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-1f70241797129cb62be0f9696dc5afe1": {
    "tokens": 1200,
    "content": "-person fluctuations in negative emotions (rather than between -person differences), we \nproceed to evaluate the model's predictive performance for each participant individually \n(nomothetic -idiographic approach). Performance decreased substantially,  with mean R² values in \nthe range of 0.06 -0.11.  However, there was considerable variability in predictive accuracy across \nparticipants. For example, when predicting negative affect , R² values ranged from 0.00 to 0.41. \nFor anger , R² achieved values as high as 0. 69, indicating that for some participants, text features \nexplained little or no variability in negative affect, while for others, text features explained a \nsignificant portion of the variability.  \nThe idiographic approach (individual models) showed similar average performance to the \nnomothetic –idiographic approach, with mean R² values ranging from 0.06 to 0.10. Figure 3 \ndemonstrates examples of high vs. low accuracy person specific models. Figure 4 further shows \nthat, despite this individual variation, the overall trend (represented by the dashed line) \ndemonstrates a generally positive relationship between predic ted and actual negative affect \nratings.  The comparison between the nomothetic –idiographic  and idiographic approaches yielded \nmixed results. Differences emerged in their ability to achieve relatively high performance for \nspecific individuals and in the proportion of significant associations across all participants . For \nnegative affect, the idiographic approach exhibited a wider range of R² values across \nparticipants, implying that this model achieved higher performance for some participants . \nHowever,  the nomothetic –idiographic approach slightly outperformed it in terms of the \nproportion of parti cipants with significant associations (55 –58% vs. 43 –48%). A similar pattern \nwas observed for sadness, suggesting that although person‐specific models can yield higher \n 15 accuracy for certain participants, the nomothetic –idiographic approach  is more robust for \nidentifying significant associations across a larger number of individuals.  \nFor anger, the nomothetic –idiographic approach not only showed greater variability in R² \nvalues but also identified more significant associations than the idiographic approach. In \ncontrast, for nervousness, the fully idiographic approach demonstrated both greater variability —\nallowing some participants to achieve R² values as high as 0.71 —and a larger  number of \nsignificant associations overall.  \nWhich Text Features Are Most Predictive in High - vs. Low -Performing Models?  \nFigure 5 displays the top 10 text features from the four best -performing subject -specific \nrandom forest models predicting negative affect. Critically, t his visualization demonstrates \nsignificant variation in the features influencing each participant's model, suggesting differences \nin the relative importance of features derived from various NLP approaches (LIWC, VADER, \nLDA, and GPT) across individuals. Though  feature importance varied across individuals, we \nalso examined whether specific patterns emerged across high - and low -performing participant -\nspecific models and whether certain NLP approaches provided more influential features in high -\nperforming models  compared to low -performing ones.  \nParticipants were divided into groups based on model performance  (R2), with the top 25% \n(n=25) classified as \"High R2\"  and the bottom 25% (n=25) as \"Low R2.\" Feature importance \nscores were analyzed separately for RF and ENR models. Importantly, p articipants with high - \nand low -performing models showed no significant difference in number of observation s (RF: \nHigh mean =72.84 , Low mean=60.64 , t=1. 69, p=0.0 9, CI=-2.31;26.71 , ENR:  High mean =62.72, \nLow mean=71.36, t=-1.26, p=0. 21, CI=-22.38;5.10) or in mean negative affect (RF: High mean \n=0.68, Low mean=0.45, t=1.85, p=0.07, CI= -0.02;0.49, ENR:  High mean =0.77, Low mean=0.65, \n 16 t=0.72, p=0. 47, CI= -0.20;0.43) but for RF high and low performing models  differed significantly \nin variability (RF: High mean =0.61, Low mean=0.40, t=3.26, p=0.002, CI=0.08;0.33, but not for \nENR:  High mean =0.59, Low mean=0.50, t=1.29, p=0. 20, CI= -0.05;0.22). Assessment number \n(\"Time\") emerged as the most important feature in both RF and ENR models, suggesting that \ntime-related variability was a relatively strong predictor of NA among  the “High R2” subjects . In \naddition, as shown in Figure 6, for the RF high -performing models (High R² group), the top 5 \nimportant variables included features from different approaches (LIWC, GPT, VADER, and \nLDA). Notably, 3 of the top 5 variables that differed the most between the High R² and Low R² \ngroups were GPT features  (e.g.. ….) , suggesting that GPT -derived variables may have been \nparticularly effective at enhancing model performance. In contrast, for high -performing ENR \nmodels, the most important predictive features were from LIWC  (e.g., xxxx) ,",
    "chunk_order_index": 5,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-ea732ed53eb9db363318a75818ff48e7": {
    "tokens": 1200,
    "content": "different approaches (LIWC, GPT, VADER, and \nLDA). Notably, 3 of the top 5 variables that differed the most between the High R² and Low R² \ngroups were GPT features  (e.g.. ….) , suggesting that GPT -derived variables may have been \nparticularly effective at enhancing model performance. In contrast, for high -performing ENR \nmodels, the most important predictive features were from LIWC  (e.g., xxxx) , with LIWC \nvariables also showing the largest differences between high - and low -performing mode ls. \nDo Closed Vocabulary, Open Vocabulary, LLM, or Combined Approaches Perform \nBest in Predicting Model Performance?  \nFigure 7 displays the performance metrics for each NLP approach used in the idiographic \nmodels.  When examining the idiographic  predictive performance  of each NLP approach  \nseparately , GPT generally showed  the highest predictive power for negative affect, sadness, \nanger, and nervousness, with R² values around 0.10 for negative affect and sadness, and \nslightly lower for anger and nervousness . These results were comparable to those achieved by \nthe idiographic models that combined all NLP approaches, except in the prediction of \nnervousness where GPT slightly outperformed the combined idiographic model . However, it  \nis important to note that while GPT showed strong R² values, it also had a higher RMSE  \ncompared to  the combined models, suggesting a higher average prediction error despite \n 17 accounting for  more variance in the outcomes . LIWC+VADER and LDA demonstrated \nconsiderably lower predictive power , with the lowest average R² values and fewer significant \nassociations across all three  emotions  and the general scale , especially for anger and \nnervousness. This highlights that the contextual understanding provided by GPT was more \neffective in capturing emotional nuances  compared to the other methods when NLP \napproaches  were tested individually. Overall, while GPT captures variability in negative \naffect (as reflected by R²), its predictions are less precise in terms of exact values (as \nindicated by RMSE). Combined models, which balance capturing variability and minimizing \nprediction errors, offer a more ro bust and reliable approach to emotion prediction.  \nDiscussion  \nIn this study, we combined multiple Natural Language Processing (NLP) approaches to \nexamine whether within -person fluctuations in emotion can be accurately tracked through text \nanalysis.  Recognizing the idiosyncratic nature of emotional communication, we compared \nidiographic models —tailored to individual patterns —with nomothetic models that capture \ncommon trends across groups . By leveraging advanced NLP and machine learning techniques to \ntrack moment -to-moment emotional changes through text analysis, our approach has the \npotential to enhance mental health monitoring, support clinical decision -making, and enable \nearly detection o f distress for timely, personalized interventions.  \nThe results showed  that, overall, nomothetic models  showed high performance in \ncontinuously predicting negative emotions (R2 range: 0.11 -0.38) . These findings align with \nprevious studies demonstrating  the utility of NLP approaches in detecting emotional states  across \nparticipants (Akhtar et al., 2019; Tanana et al., 2021) . However, while nomothetic models \nidentify general trends across participants, they do not necessarily capture nuanced within -person \nfluctuations. When we calculated performance metrics  for each participant individually  \n 18 (nomothetic -idiographic approach ), the mean models' performance declined significantly and \nshowed high between -person variability, indicating that the nomothetic models' ability to track \nwithin -person changes varies considerably from one individual to another . \nWhen we  built separate models for each participant (idiographic models)  and compared \ntheir performance to that of nomothetic -idiographic  we found that the overall predictive \nperformance was comparable between the two approaches. In addition, idiographic models \nrevealed significant variability in the text features predicting emotional states, indicating that \nindividuals express negative emotions in distinct linguistic ways.  This raises an intriguing \nquestion: if individuals differ so markedly in their predictive features, how can a nomothetic \nmodel that captures only general trends perform just as well? One plausible explanation is a \ntrade -off between statistical power and in dividual variation. While idiographic models capture \nunique, person -specific patterns, nomothetic models benefit from larger datasets that enable the \nestimation of multiple, weaker yet stable feature -emotion relationships. In other words, although \nindividu al differences  exist, the robust common patterns identified by the nomothetic approach \nappear sufficient to achieve similar predictive accuracy.  \nSupporting this interpretation, idiographic models provided highly personalized predictions \nfor some individuals (e.g., across all emotions, R² reached up to 0.84 for idiographic models vs. \n0.69 for nomothetic -idiographic models). However, nomothetic model s demonstrated greater \nconsistency across participants, successfully tracking fluctuations in negative emotions for 58% \nof individuals, compared to 48% for idiographic models.  \nWhile we are not aware of studies directly comparing nomothetic and idiographic \napproaches in text analysis, previous research comparing  their effectiveness in tracking mental \nstates using passive sensor data from smartphones and actigraphy, as well as self -reported \n 19 ecological momentary assessment (EMA), has yielded similar results (e.g., Aalbers et al., 2023; \nCheung et al., 2017; Rozet et al., 2019; for exception see Soyster et al., 2022 who found that \nnomothetic models outperfor m idiographic models ). For example,  Aalbers et al.,",
    "chunk_order_index": 6,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-4a6196f7546e6dafd49285c3dc6ee33c": {
    "tokens": 1200,
    "content": "states using passive sensor data from smartphones and actigraphy, as well as self -reported \n 19 ecological momentary assessment (EMA), has yielded similar results (e.g., Aalbers et al., 2023; \nCheung et al., 2017; Rozet et al., 2019; for exception see Soyster et al., 2022 who found that \nnomothetic models outperfor m idiographic models ). For example,  Aalbers et al., 2023  used \nsmartphone passive sensor data to predict stress levels. Their findings revealed that idiographic \nmodels demonstrated higher accuracy in tracking stress levels for some participants (Spearman’s \nρ rank -order correlation up to 1 in idiographic models vs. up to .65 in nomothetic models ). \nHowever, nomothetic models significantly predicted stress for a larger proportion of participants \n(up to 23.2% for idiographic models vs. up to 55% for nomothetic models ). Rozet et al. (2019) \nused a comparable method and initi ally found that nomothetic models performed better (i.e., \nwere more accurate) than idiographic models. However, as more data accumulated, the \nperformance of the idiographic model eventually equaled and then surpassed that of the \nnomothetic model, suggestin g that idiographic models may be a better option when sufficient \ndata is available .   \nWhen comparing specific NLP approaches, GPT outperformed other models in its ability \nto monitor fluctuations in emotional states. It achieved R² values comparable to a combined \nmodel and demonstrated significant associations with reported emotions for 57% of the \nparticipants. However, it is important to note that despite these strong R² values, GPT also \nproduced higher RMSE scores compared to the combined models. This discrepancy implies that \nwhile GPT effectively captures the overall variance in emotional states, it may be less precise in \npredicting the exact numerical values of these states. Such findings highlight the trade -off \nbetween capturing broad patterns and maintaining fine -grained precision —a balance that future \nresearch should aim to optimize , with fine -tuning offering one promising avenue for \nenhancement.  \n 20 These  findings are consistent with recent work by Rathje et al., (2024) , which reported \nhigh correlations (r = 0.66 to 0.75) between GPT -4 outputs and human ratings of emotion , \nwhereas  dictionary -based approaches showed much lower correlations (r = 0.22 to 0.30). The \nhigher correlation between GPT and human ratings in Rathje et al. (2024) study is expected as \nboth GPT and human raters evaluated the emotion directly expressed in text, whereas our study \naimed to predict participants’ self -reported emotional experiences —a task complicated by the \nfact that individuals do not always  explicitly communicate their internal states. Thus, while GPT \nand human annotation in Rathje et al. represent agreement between two raters performing the \nsame task, our approach required GPT to infer emotions that participants may not have explicitly \nexpressed.   \nFrom an applied perspective, GPT's efficiency and minimal training data requirements \nmake it particularly attractive for studies with limited datasets. However, the model's lack of \ntransparency and inability to explicitly articulate the rationale behind it s predictions remain \ncritical limitations that researchers must carefully weigh  (Feuerriegel et al., 2025) . Given these \nconsiderations, we recommend that researchers thoughtfully evaluate GPT's role in their \nanalytical pipeline, considering whether to employ it as a standalone tool or integrate it with \ncomplementary approaches. Future work should focus on deve loping methods that combine \nGPT's powerful inference capabilities with more transparent analytical approaches, potentially \noffering a more robust framework for emotion analysis in psychological research.  \nThis study is the first to use different NLP approaches to directly compare nomothetic and \nidiographic models for tracking emotional fluctuations.  Yet, f indings of this study should be \ninterpreted in light of several limitations . First, capturing the nuanced dynamics of emotional \nexpression typically requires large amounts of data. In everyday interactions, subtle emotional \n 21 changes may only become apparent with extensive exposure to an individual’s language use —\nmuch like knowing someone well enables you to discern small shifts in their mood. Therefore, \nfuture studies should strive to collect larger datasets, potentially sourc ed from daily -life \ncommunications, to enhance the ability to track and model these nuances.  \nSecond, the overall predictive accuracy was modest , leaving significant room for \nimprovement. In addition  to increasing the quantity of data, future research could benefit from \nincorporating additional modalities (e.g., vocal features, facial expression, passive sensors ) \nalongside text. Prior studies have shown that combining various modalities can enhance the \nperformance of nomothetic models of emotion  (see Gandhi et al., 2023  for a review ), this \nmultimodal approach may also improve idiographic  models. Moreover, future work should \nexplore whether individuals not only differ in the text features that predict their emotions but \nalso in the types of modalities that best capture their emotional states.  Relatedly, our analysis of \ntext features importance across high - and low -performing person -specific models did not reveal a \nsingle pattern differentiating the groups. Instead, multiple linguistic factors contributed to model \nperformance, with GPT -derived ratings emerging as robust predictors. One possible explanation \nis that individual differences, such as affective suppression, affect how em otions are conveyed in \ntext—people with high suppression may mask their emotional states, complicating accurate \npredictions. Future studies should examine whether these participants require larger datasets to \ncapture subtle nuances in their text, or if alt ernative modalities better detect changes in their \nem",
    "chunk_order_index": 7,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-ad3577c5bc8073a17cd817c05c7e86aa": {
    "tokens": 1200,
    "content": "ating the groups. Instead, multiple linguistic factors contributed to model \nperformance, with GPT -derived ratings emerging as robust predictors. One possible explanation \nis that individual differences, such as affective suppression, affect how em otions are conveyed in \ntext—people with high suppression may mask their emotional states, complicating accurate \npredictions. Future studies should examine whether these participants require larger datasets to \ncapture subtle nuances in their text, or if alt ernative modalities better detect changes in their \nemotional states.  \nThird, the current study compared idiographic and nomothetic approaches, demonstrating \nthat each has its merits. Future studies could explore hybrid approaches that integrate both \nindividual -specific and group -level information, balancing personalization with stat istical power \n 22 to enhance predictive accuracy.  Finally, the text data in our study consisted of responses to \nspecific questions, which may limit the generalizability of our findings to other types of text. \nFuture research should incorporate text from diverse sources —such as social media, \nconversational exchanges, and free -form writing —to determine whether these results extend to \nbroader contexts.   \nIn conclusion, o ur findings highlight the potential of combining NLP approaches to track \nwithin -person emotional fluctuations, demonstrating both the strengths and limitations of \nidiographic and nomothetic models. Nomothetic models effectively capture general trends, this \none-size-fits-all approach may work well for some but not for others.  Idiographic models offer a \nmore nuanced understanding by identifying person -specific features that capture the unique \ncontext of an individual's emotional expression, al though they may suffer from limited data. Our \nresults further indicate that GPT shows promise in capturing fluctuations in emotional states, \nthough further tuning is needed to enhance its precision in predicting specific ratings. Future \nresearch integratin g hybrid modeling strategies —leveraging both group -level patterns and \nindividual differences —could improve predictive accuracy and refine emotion -tracking methods. \nExpanding this work to incorporate diverse text sources and multimodal data streams may \nfurther advance the field. Ultimately, improving the ability to monitor emotions in real -time not \nonly enhances our capacity to study emotional nuances at scale in daily life but also  can help \ndevelop just -in-time interventions that go beyond identifying moments of distress to also \nconsider the specific emotions and contextual factors in which they arise, enabling more \npersonalized and effective interventions . \n  \n 23 Reference s \nAalbers, G., Hendrickson, A. T., Vanden Abeele, M. M., & Keijsers, L. (2023). Smartphone -\nTracked Digital Markers of Momentary Subjective Stress in College Students: \nIdiographic Machine Learning Analysis. JMIR mHealth and uHealth , 11, e37469.  \nAbdurahman, S., Atari, M., Karimi -Malekabadi, F., Xue, M. J., Trager, J., Park, P. S., \nGolazizian, P., Omrani, A., & Dehghani, M. (2024). Perils and opportunities in using \nlarge language models in psychological research. PNAS Nexus , 3(7), pgae245. \nhttps://doi.org/10.1093/pnasnexus/pgae245  \nAkhtar, M. S., Ghosal, D., Ekbal, A., Bhattacharyya, P., & Kurohashi, S. (2019). All -in-one: \nEmotion, sentiment and intensity prediction using a multi -task ensemble framework. \nIEEE Transactions on Affective Computing , 13(1), 285 –297. \nArun, R., Suresh, V., Veni Madhavan, C., & Narasimha Murthy, M. (2010). On finding the \nnatural number of topics with latent dirichlet allocation: Some observations . 391 –402. \nhttps://doi.org/10.1007/978 -3-642-13657 -3_43  \nBailen, N. H., Green, L. M., & Thompson, R. J. (2019). Understanding emotion in adolescents: \nA review of emotional frequency, intensity, instability, and clarity. Emotion Review , \n11(1), 63 –73. \nBarrett, L. F., Mesquita, B., Ochsner, K. N., & Gross, J. J. (2007). The experience of emotion. \nAnnu. Rev. Psychol. , 58(1), 373 –403. \nhttps://doi.org/10.1146/annurev.psych.58.110405.085709  \nBathina, K. C., Ten Thij, M., Lorenzo -Luaces, L., Rutter, L. A., & Bollen, J. (2021). Individuals \nwith depression express more distorted thinking on social media. Nature Human \nBehaviour , 5(4), 458 –466. \n 24 Beck, E. D., & Jackson, J. J. (2022). Personalized prediction of behaviors and experiences: An \nidiographic person –situation test. Psychological Science , 33(10), 1767 –1782. \nhttps://doi.org/doi.org/10.1177/09567976221093307  \nBennik, E. C., Nederhof, E., Ormel, J., & Oldehinkel, A. J. (2014). Anhedonia and depressed \nmood in adolescence: Course, stability, and reciprocal relation in the TRAILS study. \nEuropean Child & Adolescent Psychiatry , 23, 579 –586. \nBlei, D. M., Ng, A. Y., & Jordan, M",
    "chunk_order_index": 8,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-5cb7c5e98bbb0cd867a4bbf05d3d54fd": {
    "tokens": 1200,
    "content": ".1177/09567976221093307  \nBennik, E. C., Nederhof, E., Ormel, J., & Oldehinkel, A. J. (2014). Anhedonia and depressed \nmood in adolescence: Course, stability, and reciprocal relation in the TRAILS study. \nEuropean Child & Adolescent Psychiatry , 23, 579 –586. \nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine \nLearning Research , 3(Jan), 993 –1022.  \nBoyd, R. L., Ashokkumar, A., Seraj, S., & Pennebaker, J. W. (2022). The development and \npsychometric properties of LIWC -22. Austin, TX: University of Texas at Austin , 10. \nBoyd, R. L., & Schwartz, H. A. (2021). Natural language analysis and the psychology of verbal \nbehavior: The past, present, and future states of the field. Journal of Language and Social \nPsychology , 40(1), 21 –41. https://doi.org/10.1177/0261927X20967028  \nCao, J., Xia, T., Li, J., Zhang, Y., & Tang, S. (2009). A density -based method for adaptive LDA \nmodel selection. Neurocomputing , 72(7–9), 1775 –1781. \nhttps://doi.org/10.1016/j.neucom.2008.06.011  \nCarlier, C., Niemeijer, K., Mestdagh, M., Bauwens, M., Vanbrabant, P., Geurts, L., van \nWaterschoot, T., & Kuppens, P. (2022). In search of state and trait emotion markers in \nmobile -sensed language: Field study. JMIR Mental Health , 9(2), e31724. \nhttps://doi.org/10.2196/31724  \nCheung, Y. K., Hsueh, P. -Y. S., Qian, M., Yoon, S., Meli, L., Diaz, K. M., Schwartz, J. E., \nKronish, I. M., & Davidson, K. W. (2017). Are nomothetic or ideographic approaches \n 25 superior in predicting daily exercise behaviors? Methods of Information in Medicine , \n56(06), 452 –460. \nDavitz, J. R. (2013). The language of emotion . Academic Press.  \nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt, J. C., \nHecht, C., Jamieson, J., & Johnson, M. (2023). Using large language models in \npsychology. Nature Reviews Psychology , 2(11), 688 –701. \nhttps://doi.org/10.1038/s44159 -023-00241 -5 \nEichstaedt, J. C., Kern, M. L., Yaden, D. B., Schwartz, H. A., Giorgi, S., Park, G., Hagan, C. A., \nTobolsky, V. A., Smith, L. K., & Buffone, A. (2021). Closed -and open -vocabulary \napproaches to text analysis: A review, quantitative comparison, and recommend ations. \nPsychological Methods , 26(4), 398 –427. https://doi.org/10.1037/met0000349  \nFeuerriegel, S., Maarouf, A., Bär, D., Geissler, D., Schweisthal, J., Pröllochs, N., Robertson, C. \nE., Rathje, S., Hartmann, J., & Mohammad, S. M. (2025). Using natural language \nprocessing to analyse text data in behavioural science. Nature Reviews Psychology , 1–16. \nhttps://doi.org/10.1038/s44159 -024-00392 -z \nFisher, H., Fatimah, H., Pidvirny, K., Brown, H., Balkind, E., Pastro, B., & Webb, C. A. (n.d.). \nAffect dynamics in adolescent depression: Are all equilibria worth returning to?  \nFrijda, N. H. (1988). The laws of emotion. American Psychologist , 43(5), 349 –358. \nhttps://doi.org/10.1037//0003 -066x.43.5.349  \nFunkhouser, C. J., Trivedi, E., Li, L. Y., Helgren, F., Zhang, E., Sritharan, A., Cherner, R. A., \nPagliaccio, D., Durham, K., & Kyler, M. (2024). Detecting adolescent depression through \npassive monitoring of linguistic markers in smartphone communication. Journal of Child \nPsychology and Psychiatry , 65(7), 932 –941. https://doi.org/10.1111/jcpp.13931  \n 26 Gandhi, A., Adhvaryu, K., Poria, S., Cambria, E., & Hussain, A. (2023). Multimodal sentiment \nanalysis: A systematic review of history, datasets, multimodal fusion methods, \napplications, challenges and future directions. Information Fusion ,",
    "chunk_order_index": 9,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-ce12b4f787452f668adad67311181634": {
    "tokens": 1200,
    "content": ". Journal of Child \nPsychology and Psychiatry , 65(7), 932 –941. https://doi.org/10.1111/jcpp.13931  \n 26 Gandhi, A., Adhvaryu, K., Poria, S., Cambria, E., & Hussain, A. (2023). Multimodal sentiment \nanalysis: A systematic review of history, datasets, multimodal fusion methods, \napplications, challenges and future directions. Information Fusion , 91, 424 –444. \nGriffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic representation. \nPsychological Review , 114(2), 211 –244. https://doi.org/10.1037/0033 -295X.114.2.211  \nGrün, B., & Hornik, K. (2011). topicmodels: An R package for fitting topic models. Journal of \nStatistical Software , 40, 1–30. https://doi.org/10.18637/jss.v040.i13  \nHirschberg, J., & Manning, C. D. (2015). Advances in natural language processing. Science , \n349(6245), 261 –266. https://doi.org/10.1126/science.aaa8685  \nHollenstein, T., & Lanteigne, D. M. (2018). Emotion regulation dynamics in adolescence. In \nEmotion regulation  (pp. 158 –176). Routledge.  \nHouben, M., & Kuppens, P. (2020). Emotion dynamics and the association with depressive \nfeatures and borderline personality disorder traits: Unique, specific, and prospective \nrelationships. Clinical Psychological Science , 8(2), 226 –239. \nHouben, M., Van Den Noortgate, W., & Kuppens, P. (2015). The relation between short -term \nemotion dynamics and psychological well -being: A meta -analysis. Psychological \nBulletin , 141(4), 901 –930. \nHutto, C., & Gilbert, E. (2014). Vader: A parsimonious rule -based model for sentiment analysis \nof social media text . 8(1), 216 –225. https://doi.org/10.1609/icwsm.v8i1.14550  \nIliev, R., Dehghani, M., & Sagi, E. (2015). Automated text analysis in psychology: Methods, \napplications, and future developments. Language and Cognition , 7(2), 265 –290. \nhttps://doi.org/10.1017/langcog.2014.30  \n 27 Jablonka, E., Ginsburg, S., & Dor, D. (2012). The co -evolution of language and emotions. \nPhilosophical Transactions of the Royal Society B: Biological Sciences , 367(1599), \n2152 –2159.  \nKahn, J. H., Tobin, R. M., Massey, A. E., & Anderson, J. A. (2007). Measuring emotional \nexpression with the Linguistic Inquiry and Word Count. The American Journal of \nPsychology , 120(2), 263 –286. https://doi.org/10.2307/20445377  \nKuppens, P., & Verduyn, P. (2017). Emotion dynamics. Current Opinion in Psychology , 17, 22–\n26. \nLindquist, K. A. (2017). The role of language in emotion: Existing evidence and future \ndirections. Current Opinion in Psychology , 17, 135 –139. \nLundberg, S. (2017). A unified approach to interpreting model predictions. Proceedings of the \n31st International Conference on Neural Information Processing Systems , 4768 –4777.  \nManning, C. (1999). Foundations of statistical natural language processing . The MIT Press.  \nMurray, L., Israel, E. S., Balkind, E. G., Pastro, B., Lovell -Smith, N., Lukas, S. E., Forbes, E. E., \nPizzagalli, D. A., & Webb, C. A. (2023). Multi -modal assessment of reward functioning \nin adolescent anhedonia. Psychological Medicine , 53(10), 4424 –4433. \nhttps://doi.org/10.1017/ S0033291722001222  \nNeuendorf, K. A. (2017). The content analysis guidebook . sage.  \nPennebaker, J. W. (2001). Linguistic inquiry and word count: LIWC 2001 . Erlbaum.  \nPennebaker, J. W., Mehl, M. R., & Niederhoffer, K. G. (2003). Psychological aspects of natural \nlanguage use: Our words, our selves. Annual Review of Psychology , 54(1), 547 –577. \nhttps://doi.org/10.1146/annurev.psych.54.101601.145041  \n 28 Rathje, S., Mirea, D. -M., Sucholutsky, I., Marjieh, R., Robertson, C. E., & Van Bavel, J. J. \n(2024). GPT is an effective tool for multilingual psychological text analysis. Proceedings \nof the National Academy of Sciences , 121(34), e2308950121.  \nRozet, A., Kronish, I. M., Schwartz, J. E., & Davidson, K",
    "chunk_order_index": 10,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-b2edb42b5bbacf1e20b13d9428d50790": {
    "tokens": 1200,
    "content": "Rathje, S., Mirea, D. -M., Sucholutsky, I., Marjieh, R., Robertson, C. E., & Van Bavel, J. J. \n(2024). GPT is an effective tool for multilingual psychological text analysis. Proceedings \nof the National Academy of Sciences , 121(34), e2308950121.  \nRozet, A., Kronish, I. M., Schwartz, J. E., & Davidson, K. W. (2019). Using machine learning to \nderive just -in-time and personalized predictors of stress: Observational study bridging the \ngap between nomothetic and ideographic approaches. Journal of Medical Internet \nResearch , 21(4), e12910.  \nSchoevers, R., Van Borkulo, C., Lamers, F., Servaas, M., Bastiaansen, J., Beekman, A., Van \nHemert, A., Smit, J., Penninx, B., & Riese, H. (2021). Affect fluctuations examined with \necological momentary assessment in patients with current or remitted depress ion and \nanxiety disorders. Psychological Medicine , 51(11), 1906 –1915. \nhttps://doi.org/10.1017/S0033291720000689  \nSivakumar, S., Videla, L. S., Kumar, T. R., Nagaraj, J., Itnal, S., & Haritha, D. (2020). Review \non word2vec word embedding neural net . 282 –290. \nSoyster, P. D., Ashlock, L., & Fisher, A. J. (2022). Pooled and person -specific machine learning \nmodels for predicting future alcohol consumption, craving, and wanting to drink: A \ndemonstration of parallel utility. Psychology of Addictive Behaviors , 36(3), 296 –306. \nSun, J., Schwartz, H. A., Son, Y., Kern, M. L., & Vazire, S. (2020). The language of well -being: \nTracking fluctuations in emotion experience through everyday speech. Journal of \nPersonality and Social Psychology , 118(2), 364. https://doi.org/10.1037/pspp0000244  \nTackman, A. M., Sbarra, D. A., Carey, A. L., Donnellan, M. B., Horn, A. B., Holtzman, N. S., \nEdwards, T. S., Pennebaker, J. W., & Mehl, M. R. (2019). Depression, negative \nemotionality, and self -referential language: A multi -lab, multi -measure, and multi -\n 29 language -task research synthesis. Journal of Personality and Social Psychology , 116(5), \n817–834. https://doi.org/10.1037/pspp0000187  \nTanana, M. J., Soma, C. S., Kuo, P. B., Bertagnolli, N. M., Dembe, A., Pace, B. T., Srikumar, V., \nAtkins, D. C., & Imel, Z. E. (2021). How do you feel? Using natural language processing \nto automatically rate emotion in psychotherapy. Behavior Research Methods , 1–14. \nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and \ncomputerized text analysis methods. Journal of Language and Social Psychology , 29(1), \n24–54. https://doi.org/10.1177/0261927X09351676  \nvan Loon, A. (2022). Three families of automated text analysis. Social Science Research , 108, \n102798. https://doi.org/10.1016/j.ssresearch.2022.102798  \n \n  \n 30  \n  Fig. 1  \nEstimation of the most preferable number of topics for LDA model  \n\n 31   \nTopic \nnumber  Topic interpretation   \n1 Social & Evening Activities:  \nfocused on social interactions and \nnighttime activities with friends  friend, talk, sleep, last, night, last_night, see, \ntime, morning, hang, wake, late, talk_friend, \nboyfriend, hang_friend,  \n2 Academic Life:  centered on school -\nrelated activities and academic \nresponsibilities  homework, school, think, class, finish, test, \nnow, take, tomorrow, math, studi, right, \nfinals, essay, stress  \n3 Home activities : capturing leisure \nactivities, particularly around media \nconsumption and family time  watch, noth ing, eat, dinner, play, game, show, \nfamily, movie, tv, ate, video, eat_dinner, \nwatch_tv, favorit e, watch_movi e \n4 Family Interactions:  reflecting \nfamily relationships and dynamics  mom, think, fun, sister, new, dad, brother, \nroom, read, make , made, book, someth ing, \nfight, clean                 \n5 Daily Activities:  representing \nroutine daily activities like meals  go, went, home, walk, outsid e, lunch, food, \ndrive, music, back, listen, shop, car, hurt, \naround, dog,  \n6 Personal States & Obligations:  \ndescribing emotional states, needs, \nand work -related responsibilities  get, work, feel, want, today, day, need, good, \nthink, just, felt, like, done, tire d, sick,                     Table 1  \nThe 15",
    "chunk_order_index": 11,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-5d458f7ddd2db73adb255a1d332d56fc": {
    "tokens": 1200,
    "content": "go, went, home, walk, outsid e, lunch, food, \ndrive, music, back, listen, shop, car, hurt, \naround, dog,  \n6 Personal States & Obligations:  \ndescribing emotional states, needs, \nand work -related responsibilities  get, work, feel, want, today, day, need, good, \nthink, just, felt, like, done, tire d, sick,                     Table 1  \nThe 15 most frequent words within the six topics extracted using LDA  \n 32  \nTable 2 \nComparison of Idiographic, Nomothetic, and Nomothetic –Idiographic Model Performance for \nNegative Affect, Sadness, Anger, and Nervousness  Using Random Forest and Elastic Net  \n \n \n    Nomothetic  Nomothetic -idiographic  Idiographic  \n    R2 R  RMS\nE R2 R2 range  RMS\nE R2 R2 range  RMS\nE \nNegativ\ne Affect  Rando\nm \nForest  .38 .62 0.55 .10 \n(.10)  .00; .41  \nSig.n= \n53/97  0.52 \n(0.22)  .10 \n(.12)  .00; .47  \nSig.n= \n42/97  0.47 \n(0.20)  \n  Elastic \nnet \n .17 .41 0.64 .11 \n(.09)  .00; .39  \nSig.n= \n56/97  0.61 \n(0.27)  .10 \n(.12)  .00; .53  \nSig.n= \n47/97  0.48 \n(0.20)  \nSad Rando\nm \nForest  .33 .58 0.75 .10 \n(.12)  .00; 0.53  \nSig.n= \n48/96  0.73 \n(0.27)  .10 \n(.13)  .00; .46  \nSig.n= \n38/94  0.68 \n(0.22)  \n  Elastic \nnet \n .14 .37 0.85 .09 \n(.09)  .00; .42  \nSig.n= \n51/96  0.82 \n(0.32)  .10 \n(.11)  .00; .51 \nSig.n= \n46/94  0.69 \n(0.23)  \nAngry  Rando\nm \nForest  .20 .45 .69 .09 \n(.12)  .00; .69  \nSig.n= \n37/90  0.63 \n(0.33)  .06 \n(.07)  .00;.33 \nSig.n= \n27/85  0.64 \n(0.29)  \n  Elastic \nnet \n .13 .36 .72 .07 \n(.07)  .00; .45  \nSig.n= \n49/90  0.88 \n(0.32)  .07 \n(.08)  .00; .43 \nSig.n= \n39/85  0.66 \n(0.31)  \nNervous  Rando\nm \nForest  .24 .49 .85 .06 \n(.08)  .00; .44  \nSig.n= \n33/95  0.81 \n(0.29)  .08 \n(.12)  .00; 0.71 \nSig.n= \n33/94  0.75 \n(0.27)  \n  Elastic \nnet \n .11 .33 .92 .07 \n(.07)  .00; .35  \nSig.n= \n42/95  0.88 \n(0.33)  .09 \n(.12)  .00; 0.84 \nSig.n= \n43/94  0.76 \n(0.29)  \n 33   \nFigure 2  \nFeature importance of nomothetic RF and ENR models  \n \nNA GPT \nHappy GPT  \nFocus past (LIWC)  \nCause (LIWC)  \nInterested GPT  \nSad GPT  \nInsight (LIWC)  \nAdverb (LIWC)  \nCognitive (LIWC)  \nThought Length  \nEmotion neg (LIWC)  \nDeterminers   (LIWC ) \nTopic 4 (LDA)  \nNervous GPT  \nAchieve (LIWC)  \nPhysical (LIWC)  \nCompound (VADER)  \nFemale (LIWC)  \nTime  \nConjunction (LIWC)  \n \n \n \nID \nNA GPT \nSad GPT  \nTime  \nFocus past (LIWC)  \nCause (LIWC)  \nThought Length  \nConjunction (LIWC)  \nAdverb (LIWC)  \nTopic 1 (LDA)  \nAnalytic (LIWC)  \nPA GPT  \nDictionary words  (LIWC)   \nHappy  GPT \nNervous  GPT \nWord count  (LIWC)  \nBig Word s (LIWC)  \nWork  (LIWC)  \nFunction  (LIWC)  \nTopic 2 (LDA)  \n \n \n 34   \nFigure 3  \nExamples of Person -Specific (Idiographic) Predictions of Negative Affect for High - and \nLow-Performance Models  \n 35        Figure 4 \nPerson -Specific (Idiographic) Predictions of Negative Affect  (a) Random Forest (b) Elastic Net   \n  \n\n 36  \n \n \n \n  \nFigure 5 \nTop 10 Predictive Features Across Four Example Person -Specific  Best-Performing \nRandom Forest Models for Negative Affect Prediction   \n 37       \nFigure 6  \nGroup differences in feature importance predicting negative affect: random forest (",
    "chunk_order_index": 12,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-f2728922ca9bd72a109970a4edfe8cac": {
    "tokens": 1151,
    "content": ") Predictions of Negative Affect for High - and \nLow-Performance Models  \n 35        Figure 4 \nPerson -Specific (Idiographic) Predictions of Negative Affect  (a) Random Forest (b) Elastic Net   \n  \n\n 36  \n \n \n \n  \nFigure 5 \nTop 10 Predictive Features Across Four Example Person -Specific  Best-Performing \nRandom Forest Models for Negative Affect Prediction   \n 37       \nFigure 6  \nGroup differences in feature importance predicting negative affect: random forest (RF) \nand elastic regularization  (ENR)  \n 38   Figure 6  \nGroup differences in feature importance  predicting negative affect : random forest (RF) \nand elastic regularization  (ENR)  \n\n 39  \n \n \n \n \n \n    LIWC+VADER  LDA  GPT  \n    R2 R2 \nrange  RMS\nE R2 R2 range  RMS\nE R2 R2 \nrange  RMS\nE \nNegativ\ne Affect  Rando\nm \nForest  .05 \n(.08)  .00;.42  \nSig.n= \n31/97 0.49 \n(0.21)  .07 \n(.09)  .00;.48  \nSig.n= \n36/97  0.49 \n(0.21)  .10 \n(.08)  .00;.34 \nSig.n= \n55/97  0.85 \n(.20)  \n  Elastic \nnet \n .07 \n(.10)  .00;.55  \nSig.n= \n36/97  0.49 \n(0.22)  .06 \n(.07)  .00;.44  \nSig.n= \n35/97  0.50 \n(0.22)  \nSad Rando\nm \nForest  .05 \n(.08)  .00;.38  \nSig.n= \n24/95 0.70 \n(0.25)  .05 \n(.08)  .00;.46 \nSig.n= \n25/95  0.71 \n(0.26)  .10 \n(.10)  .00; .54  \nSig.n= \n55/96  1.11 \n(0.26)  \n  Elastic \nnet \n .07 \n(.08)  .00;.44  \nSig.n= \n42/95 0.70 \n(0.26)  .07 \n(.07)  .00;.29  \nSig.n= \n45/95  0.71 \n(0.27)   \nAngry  Rando\nm \nForest  .04 \n(.06)  .00;.28  \nSig.n= \n15/70 0.72 \n(0.29)  .03 \n(.05)  .00;.58 \nSig.n= \n12/85  0.68 \n(0.32)  .09 \n(.10)  .00;.44 \nSig.n= \n45/90  0.93 \n(0.34)  \n  Elastic \nnet \n .05 \n(.07)  .00;.27  \nSig.n= \n26/70 0.73 \n(0.31)  .07 \n(.07)  .00;.28  \nSig.n= \n43/84  0.67 \n(0.32)    \nNervous  Rando\nm \nForest  0.04 \n(0.06)  .00;.29  \nSig.n= \n19/86 0.80 \n(0.26)  .05 \n(.08)  .00;.53  \nSig.n= \n29/94  \n 0.77 \n(0.28)  .06 \n(.06)  .00;.29 \nSig.n= \n40/95  1.36 \n(0.22)  \n  Elastic \nnet \n .05 \n(.07) .00;.35  \nSig.n= \n25/86 .81 \n(28) .05 \n(.05)  00;.23  \nSig.n= \n36/94  \n 0.76 \n(0.28)    Table 3 \nComparison of Idiographic Model Performance for Negative Affect, Sadness, Anger, and Nervousness \nUsing LIWC+VADER, LDA, and GPT  \n \n 40  \n \n \n  \nFigure 7 \nPredictive Performance  of Idiographic Model Across NLP Approaches  \n 41 Supplement  \n  \n\n 42   \n 43  \n \n \n \n\n 44   \n\n 45  \n \nBERT (Bidirectional Encoder Representations from Transformers),  in contrast, is a \nbidirectional model that takes into account both the left and right context of a sentence, making it \nmore suited for tasks like sentiment analysis and natural language understanding (NLU). BERT's \narchitecture enables it to understand the  full context of a sentence by analyzing words from all \ndirections simultaneously, which is particularly useful for tasks where understanding the \nmeaning of a phrase within its broader context is essential.  Both GPT -4 and BERT are pre -\ntrained on large text  datasets. However, while GPT -4 benefits from a significantly larger dataset, \nBERT’s bidirectional architecture makes it better suited for tasks requiring a deep understanding \nof text context, such as sentiment analysis. In this study, BERT model, specifically trained for \nemotion classification (\"bhadresh -savani/bert -base-go-emotion\"), is used to tokenize the \nconcatenated text inputs and generate emotion predictions.",
    "chunk_order_index": 13,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-09b11a8aff7c3c7f4f71e22015044e39": {
    "tokens": 51,
    "content": ", such as sentiment analysis. In this study, BERT model, specifically trained for \nemotion classification (\"bhadresh -savani/bert -base-go-emotion\"), is used to tokenize the \nconcatenated text inputs and generate emotion predictions.",
    "chunk_order_index": 14,
    "full_doc_id": "doc-0c59b17eae5d162af485322277b60bfb"
  },
  "chunk-9cb9e3bb77023a09bbffc3d3f1095a31": {
    "tokens": 1200,
    "content": "Page 1: Model Performance Analysis \nUI & Data Controls: \n Title & Layout: The page is set up with a centered, wide layout. The left column contains \ncontrols and dropdowns while the right column displays the visualization and summary table. \n Controls: \no NLP Approach Dropdown: Options include LDA, GPT, COMBINED, and LIWC. \no Idiographic/Nomothetic Dropdown: Changes based on the NLP approach (for GPT, \ndefaults to “Nomothetic” for data availability). \no Outcome Dropdown: Selects the outcome (e.g., Negative Affect, Angry, Nervous, Sad). \nFor GPT, outcome values are standardized (e.g., “negative affect” becomes “na”) and \nfiltered from the file. \nGraph Details: \n Graph Type: Violin Plot \n Axes: \no X-Axis: Represents the “ML Model.” In GPT mode, it shows a single category (“GPT”), \nwhile in other cases it shows two categories: “Elastic Net (en)” and “Random Forest \n(rf).” \no Y-Axis: Represents the R² values, showing the distribution of model performance. \n Data & Filtering: \no GPT: Loads from modelfit_gpt_all.csv filtered by outcome and (if available) the \nnomothetic/idiographic value. \no Non-GPT Models: Loads separate CSV files for Elastic Net and Random Forest; these \ndatasets are concatenated. \n Colors: \no GPT Mode: The violin is rendered in blue. \no Non-GPT: Elastic Net is shown in red, and Random Forest is shown in blue. The \ncolors help differentiate the model types and make it easier to visually compare their \nR² distributions. \n Additional Features: The violin plot includes boxplots and all data points are shown, offering \ninsight into both central tendency and spread. \nSummary Table: \n Content: A table summarizing key statistics per model including the mean (with standard \ndeviation), count (N), range (min to max R²), and the count of p-values below 0.05. \n Calculation: Each metric is computed on the filtered dataset and then presented in a neat \ntabular format. \n \nPage 2: Data Table View – Model Performance per Participant \nUI & Data Controls: \n Layout: The page uses a centered, wide layout with controls in the left column and a data table \nin the right column. \n Controls: \no NLP Approach, Idiographic/Nomothetic, ML Model, and Outcome Dropdowns: These \ndetermine which CSV file is loaded. For GPT, the outcome is chosen from unique \nvalues in the file; otherwise, the outcome is standardized (e.g., “negative affect” to \n“na”). \nData Table: \n Table Type: Interactive Data Table \n Displayed Columns: Typically includes id, participant, r, r2, rmse, and p_value. \n Purpose: Presents participant-level performance metrics from the selected file. \n Axes Explanation: \no Unlike a graph, the table shows rows for each participant and columns for each metric. \n Color and Formatting: \n \n o No explicit colors are used in the table; however, column headers and data formatting \nhelp in quick reading. \n Interaction: The table is scrollable with a fixed height (600px) and width (1200px). \n \nPage 3: True vs Predicted NA Levels \nUI & Data Controls: \n Title & Refresh: The title “True vs Predicted NA Levels” is displayed along with a “Clear All” \nbutton that resets the session state. \n Dynamic Graph Sections: The user can add multiple graph sections; each section has its own \ncontrols for outcome, ML model, and participant selection. \nGraph Details: \n Graph Type: Line Chart \n Axes: \no X-Axis: Represents time (the “time” column from the dataset). \no Y-Axis: Represents NA levels; two lines are plotted —one for the actual (true) NA \nlevels and one for the predicted estimates. \n Data & Filtering: \no Data is loaded from a CSV file whose filename is constructed based on the selected \nML model, outcome, and nomothetic/idiographic value. \no After selecting a participant, the data is filtered to include only rows for that \nparticipant. \n Colors: \no Actual NA Levels: Plotted in teal with a solid line. \no Predicted NA Levels: Plotted in red using a dashed line. \no Correlation Calculation: The correlation coefficient between the actual and predicted \nNA values is computed and displayed in the title. \n Additional Features: \no The graph has fixed dimensions (900 px wide, 400 px high) to ensure consistency, and \ntooltips show detailed information when hovering. \n \nPage 4: Best Model Performance per Participant \nUI & Data Controls: \n Layout: The page features controls on the left (including checkboxes and dropdowns) and a \nresults section on the right. \n Controls: \no Checkbox: “Include both Elastic Net and Random Forest” – when checked, both \nmodels are compared; when unchecked, a dropdown allows selection of one model.  \no Outcome Dropdown: Allows selection of an outcome (e.g., Negative Affect, Angry, \nNervous, Sad). \nData Processing: \n Aggregation: Data from multiple CSV files (covering different NLP approaches and both \nidiographic and nomothetic cases) are merged and filtered. \n Selection: For each participant, the record with the highest R² is selected. \nDisplayed Elements: \n Data Table: \no Content: Shows the best performance for each participant, including Participant, ML \nModel (",
    "chunk_order_index": 0,
    "full_doc_id": "doc-362820e119661525efc80e7426cbea0c"
  },
  "chunk-545942d45770473344999994f7056e47": {
    "tokens": 1200,
    "content": "an outcome (e.g., Negative Affect, Angry, \nNervous, Sad). \nData Processing: \n Aggregation: Data from multiple CSV files (covering different NLP approaches and both \nidiographic and nomothetic cases) are merged and filtered. \n Selection: For each participant, the record with the highest R² is selected. \nDisplayed Elements: \n Data Table: \no Content: Shows the best performance for each participant, including Participant, ML \nModel (if both are included), Nomothetic/Idiographic, NLP Approach, R², RMSE, P \nValue, and Counts. \n Pie Charts: \no Graph Types: Pie Charts \no Pie Chart 1 (Nomothetic vs. Idiographic): \n Slices: Represent counts of participants per category. \n \n  Colors: Use Plotly’s default discrete colors to differentiate the groups. \no Pie Chart 2 (NLP Approaches): \n Slices: Represent the frequency of each NLP approach; “comb” is re -labeled as \n“All text features combined.” \n Colors: Also use discrete color mapping. \no Pie Chart 3 (ML Models): \n Shown only if both models are included. \n Slices: Represent the counts for Elastic Net and Random Forest. \n Colors: Defined via a discrete map (Elastic Net in red, Random Forest in blue). \n Axes Explanation for Pie Charts: \no Pie charts do not have traditional axes; the slices’ sizes represent proportions or counts.  \n Legend & Layout: \no The charts are arranged in columns with legends and centered titles for clarity. \n \nPage 5: Feature Importance Heatmap (SHAP Values) \nUI & Data Controls: \n Layout: The page uses a two-column layout with controls in the right column and the heatmap \nvisualization in the left column. \n Controls: \no Outcome and Model Dropdowns: Users select the outcome (e.g., Negative Affect, \nAngry, etc.) and the ML model (Elastic Net (EN) or Random Forest (RF)). \no Participant Multi-select: Allows selection of specific participants, or an “All” option \ncan select every participant. \no Dynamic Symmetric Slider: A slider allows users to set a symmetric threshold for \nfeature importance values. The slider is linked to a callback that forces the two \nhandles to be opposites (e.g., if one is set to –0.005, the other becomes 0.005). \no Info Box: A message box below the slider explains which SHAP importance values \nwill be included (those outside the threshold) and which will be filtered out. \nGraph Details: \n Graph Type: Heatmap \n Axes: \no X-Axis: Represents the selected participants. When “All” is chosen, the x-axis shows a \nsingle label (“All Participants”); otherwise, it shows individual participant names.  \no Y-Axis: Represents the features (variables) that passed the importance threshold. \n Data Processing: \no Data is loaded from a CSV (named by model and outcome). After grouping and \naveraging, the data is pivoted so that features form the rows and participants the \ncolumns. \no The pivoted data is filtered based on the importance threshold. \n Colors: \no Color Scale: A custom color scale is used: \n Deep Blue represents strong negative SHAP values. \n White represents neutral importance. \n Deep Red represents strong positive SHAP values. \no Feature Label Colors: Each feature label on the y-axis is colored according to its NLP \nmethod (using a predefined color map such as LIWC in red, GPT in blue, etc.). \n Layout Adjustments: \no The heatmap’s height and width are dynamically set based on the number of features \nand participants. \no A legend below the graph (built using HTML) explains the color mapping for the NLP \nmethods. \n \nPage 6: Feature Importance per Participant (SHAP Value) \n \n UI & Data Controls: \n Layout: Similar to Page 5, this page uses a two-column layout with controls in the right \ncolumn and the visualization in the left column. \n Controls: \no Outcome & Model Dropdowns: Allow the user to select the outcome and ML model \n(with outcome normalized). \no Participant Dropdown: Enables the selection of a single participant from the loaded \ndataset. \no Performance Metrics Loading: A secondary CSV is loaded to extract performance \nmetrics (R² and RMSE) for the selected participant, which are later shown in the graph \ntitle. \no Dynamic Symmetric Slider: As on Page 5, a slider is provided for setting the threshold \nfor feature importance. It uses a symmetric range and displays an info box. \nGraph Details: \n Graph Type: SHAP Summary Scatter Plot with Vertical Lines \n Axes: \no X-Axis: Represents the SHAP (feature importance) values. \no Y-Axis: Represents features. The y-axis values are evenly spaced for each feature, and \nfeature names are used as tick labels. \n Data Processing: \no Data is filtered for the selected participant and then filtered based on the chosen \nimportance threshold. \no The data is sorted by NLP method (and importance) and assigned evenly spaced y-axis \npositions. \n Colors: \no Vertical Lines & Markers: Each feature’s importance is shown with a line extending \nfrom 0 to the SHAP value, with a dot at the end. The color of the line and dot \ncorresponds to the feature’s NLP method (e.g., LIWC is red, GPT is blue, etc.). \no Legend: Additional dummy traces ensure every NLP method appears in the legend. \n Additional Features: \no The",
    "chunk_order_index": 1,
    "full_doc_id": "doc-362820e119661525efc80e7426cbea0c"
  },
  "chunk-db43bbbeae39de076dab95364787c09b": {
    "tokens": 1003,
    "content": "y-axis \npositions. \n Colors: \no Vertical Lines & Markers: Each feature’s importance is shown with a line extending \nfrom 0 to the SHAP value, with a dot at the end. The color of the line and dot \ncorresponds to the feature’s NLP method (e.g., LIWC is red, GPT is blue, etc.). \no Legend: Additional dummy traces ensure every NLP method appears in the legend. \n Additional Features: \no The plot title includes the participant’s name along with their performance metrics (R² \nand RMSE). \no A dashed vertical line at x=0 helps delineate positive from negative importance. \n \nPage 7: Feature Importance Analysis \nUI & Data Controls: \n Layout: The page is divided into a left column for controls and a right column for the graphs. \n Controls: \no Dropdowns: \n ML Model & Outcome: Users select the model and outcome (with outcome \nstandardized). \no Checkbox: “Include the variable 'Time'” (default is checked). \no Participant Filtering: A slider lets users select the percentage of participants in each \ngroup based on performance (R²). \no Minimum Variable Occurrence Slider: Sets a threshold for how many participants \nmust have a feature for it to be included. \n Data Processing: \no Performance Data: Loaded from a performance CSV, participants are split into high \nand low R² groups. \no Feature Importance Data: Loaded from another CSV, with an option to exclude the \n“Time” variable. \no Aggregation: The absolute mean importance for each feature is computed separately \nfor high and low R² groups. The absolute mean difference is also calculated. \n \n Graph Details: \n Graph Type: Bar Charts (two separate charts) \n First Bar Chart – High vs. Low R² Groups: \no Axes: \n X-Axis: Represents the absolute mean value for each group. \n Y-Axis: Lists the features (sorted by the high R² group’s values) with labels \ncolor-coded by NLP method. \no Bars: \n Two sets of bars for each feature: one for the high R² group (colored red) and \none for the low R² group (colored turquoise). \n Second Bar Chart – Absolute Mean Difference: \no Axes: \n X-Axis: Represents the absolute mean difference between the high and low R² \ngroups. \n Y-Axis: Lists the top features (sorted by difference) with color-coded labels. \no Bars: \n A single set of gray bars represents the difference. \n Colors: \no The bar colors (red, turquoise, and gray) are chosen to clearly contrast the groups, and \nthe y-axis labels are enhanced with HTML to include NLP method colors. \nLegend: \n An HTML-based legend at the bottom explains the NLP method color coding. \n \nPage 8: Common Top Predictive Features \nUI & Data Controls: \n Layout: This page uses a sidebar layout. \no Left Sidebar (Controls): \n Model Selection Dropdown: Allows the choice between Elastic Net (EN) and \nRandom Forest (RF). \n Sliders: \n Number of Features per Participant (determines how many top \nfeatures per participant are selected). \n Number of Variables in Figure (sets how many variables appear in the \naggregate view). \n SHAP Value Threshold (filters features based on a minimum absolute \nSHAP value). \n Checkbox: “Use ABS values only” toggles the data source and indicates that \nonly absolute values are considered. \n Data Processing: \no For each emotion (na, sad, angry, nervous), a CSV file is loaded based on the selected \nmodel and whether ABS values are used. \no Each participant’s top features (based on the SHAP threshold) are aggregated. The \ncount of participants selecting each feature is calculated, and the SHAP sign (Positive \nor Negative) is determined. \n Graph Details: \no Graph Type: Stacked Horizontal Bar Charts \no Axes: \n X-Axis: Represents the “count” (or percentage of participants) in which the \nfeature is selected. \n Y-Axis: Represents the feature names, with labels color-coded based on their \nassociated NLP method. \no Bar Colors: \n Bars are split by SHAP sign: \n \n  Positive Values: Shown in a teal-like color (rgb(0,182,185)). \n Negative Values: Shown in a red hue (rgb(255,79,82)). \n Layout of Graphs: \no The four emotion-specific bar charts are arranged in a 2×2 grid in the main area. \n Legend: \no A legend below the charts (rendered in HTML) explains the NLP method colors.",
    "chunk_order_index": 2,
    "full_doc_id": "doc-362820e119661525efc80e7426cbea0c"
  }
}